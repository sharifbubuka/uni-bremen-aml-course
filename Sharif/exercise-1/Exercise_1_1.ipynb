{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditional Probabilities\n",
    "You know the University of Bremen has 18,631 students, of which 6,671 are in natural sciences and engineering (see https://www.uni-bremen.de/en/university/profile/facts-figures ). Three-quarters of your friends in the natural sciences like mate (a beverage) from your personal experience. \n",
    "You are curious if you can determine how likely someone studies in this field, given they like mate. Therefore, you conduct a quick experiment in the mensa and ask at random tables the field and how much they like mate. \n",
    "\n",
    "The following matrix describes your data. The first column describes if the person studies natural sciences (or not) and the second how much they like mate (scale from -2 to 2, higher=likes better, neutral is not allowed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  1],\n",
       "       [ 0, -1],\n",
       "       [ 0,  1],\n",
       "       [ 0, -1],\n",
       "       [ 1,  1],\n",
       "       [ 0,  1],\n",
       "       [ 0, -2],\n",
       "       [ 0, -1]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questionaire_mate = np.array([[True, 1], [False, -1], [False, 1], [False, -1], [True, 1], [False, 1], [False, -2], [False, -1]])\n",
    "\n",
    "questionaire_mate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a person likes mate, how likely are they to study in the natural sciences?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability that a person studies natural sciences, P(A): 0.3580591487306103\n",
      "Probability that a person likes mate, P(B): 0.5\n",
      "Number of students who study natural sciences, num_A:  2\n",
      "Number of students who like mate given they study natural sciences, num_B_given_A:  2\n",
      "Probability that a person likes mate given they study natural sciences, P_B_given_A: 1.0\n",
      "Probability of studying in the natural sciences given that the person likes mate, P_A_given_B: 0.7161182974612206\n"
     ]
    }
   ],
   "source": [
    "# P(A) = Probability that a person studies natural sciences\n",
    "P_A = 6671/18631\n",
    "print(\"Probability that a person studies natural sciences, P(A):\", P_A)\n",
    "\n",
    "#P (B) = Probability that a person likes mate\n",
    "P_B = np.sum(questionaire_mate[:, 1] > 0) / questionaire_mate.shape[0] \n",
    "print(\"Probability that a person likes mate, P(B):\", P_B)\n",
    "\n",
    "# P(A|B) = Probability that a person studies natural sciences, given they like mate\n",
    "# Using Bayes Theorem; P(A|B) = (P(B|A)* P(A)) /P(B)\n",
    "\n",
    "# We have P(A) and P(B), lets find P(B|A) and substitute it in our formular\n",
    "natural_science_students = questionaire_mate[questionaire_mate[:, 0] == 1]\n",
    "print(\"Number of students who study natural sciences, num_A: \", natural_science_students.shape[0])\n",
    "\n",
    "num_like_mate_given_study_natural_sciences = np.sum(natural_science_students[:, 1] > 0)\n",
    "print(\"Number of students who like mate given they study natural sciences, num_B_given_A: \", num_like_mate_given_study_natural_sciences)\n",
    "\n",
    "# Therefore, probability that person likes mate given they study natural sciences, P(B|A);\n",
    "P_B_given_A = num_like_mate_given_study_natural_sciences / natural_science_students.shape[0]\n",
    "print(\"Probability that a person likes mate given they study natural sciences, P_B_given_A:\", P_B_given_A)\n",
    "\n",
    "# Hence;\n",
    "P_A_given_B = (P_B_given_A * P_A) / P_B\n",
    "\n",
    "print(\"Probability of studying in the natural sciences given that the person likes mate, P_A_given_B:\", P_A_given_B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximum Likelihood Estimation\n",
    "A Gaussian normal distribution can be fitted by applying the Maximum Likelihood Estimation to determine the best parameters for explaining a given dataset. This is equivalent to calculating the mean (and variance) on the dataset directly; why?\n",
    "\n",
    "The Gaussian normal distribution is given as follows:\n",
    "\n",
    "$$\n",
    "N(\\mu,\\sigma) = \\frac{1}{\\sigma \\sqrt{2\\pi}} \n",
    "e^{-\\frac{1}{2}(\\frac{x-\\mu}{\\sigma})^2}\n",
    "$$\n",
    "\n",
    "Hint: The partial derivative is easier to compute when using a log likelihood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Kullback-Leibler Divergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$D_{KL}(P|Q) = \\sum_x P(x)log(\\frac{P(x)}{Q(x)})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Calculate the KL divergence for two discrete distributions $P$ and $Q$ over events $A,B,C$. \n",
    "Calculate $D_{KL}(P|Q)$ and $D_{KL}(Q|P)$ and compare! \n",
    "\n",
    "| Distribution | A | B | C |\n",
    "| --- | --- | --- | --- |\n",
    "| P | 0.5 | 0.3 | 0.2 |\n",
    "| Q | 0.4 | 0.2 | 0.4 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KL(P|Q) 0.0945818719775651\n",
      "KL(Q|P) 0.10690843007666134\n"
     ]
    }
   ],
   "source": [
    "p = [0.5,0.3,0.2]\n",
    "q = [0.4,0.2,0.4]\n",
    "\n",
    "# implement or calculate:\n",
    "\n",
    "def KL(a, b):\n",
    "    a = np.asarray(a)\n",
    "    b = np.asarray(b)\n",
    "    \n",
    "    if len(a) != len(b):\n",
    "        raise ValueError(\"Arrays must have the same length.\")\n",
    "    \n",
    "    return np.sum(np.where(a != 0, a * np.log(a/b), 0))\n",
    "\n",
    "print(\"KL(P|Q)\", KL(p, q))\n",
    "print(\"KL(Q|P)\", KL(q, p))\n",
    "\n",
    "# Inconsistent results with kl_div from scipy.special"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$D_{KL}(P|Q)$ is less than $D_{KL}(Q|P)$, indicating that the divergence from Q to P is higher than the divergence from P to Q."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) For this task, assume for simplicity that $P$ and $Q$ are discrete distributions over two events $A,B$. \n",
    "\n",
    "i) For a given $P$, what $Q_{min}$ minimizes $D_{KL}(P|Q)$? Justify your answer!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "**ANSWER**\n",
    "\n",
    "Consider the following; $p_A$ = $P(A)$, $p_B$ = $P(B)$, $q_A$ = $Q(A)$ and $q(B)$ = $Q(B)$\n",
    "\n",
    "Given that we are only considering two events; A and B, therefore, the following constraint holds; $q_A + q_B = 1$\n",
    "\n",
    "To get the value of $q_A$ that minimizes the $D_KL(P|Q)$, set the derivative of the $D_KL(P|Q)$ with respect to $q_A$ to 0, and solve for $q_A$.\n",
    "\n",
    "$\\frac{d}{d_(qA)}D_KL(P||Q) = \\frac{d}{d_(qA)}[p_A log(\\frac{p_A}{q_A}) + p_B log(\\frac{p_B}{q_B})]$\n",
    "\n",
    "Using the chain rule:\n",
    "\n",
    "$\\frac{d}{d_(qA)}D_KL(P||Q) = -\\frac{p_A}{q_A} + \\frac{p_B}{1- q_A}$ (See calculation <a href=\"#1b1\">here</a>.)\n",
    "\n",
    "Setting this to zero, and solving for $q_A$;\n",
    "\n",
    "$-\\frac{p_A}{q_A} + \\frac{p_B}{1- q_A} = 0$\n",
    "\n",
    "$-p_A(1 - q_A) + p_B q_A = 0$\n",
    "\n",
    "$-p_A + p_A q_A + p_Bq_A = 0$\n",
    "\n",
    "$(p_A + p_B)qA = p_A$\n",
    "\n",
    "$q_A = \\frac{p_A}{p_A + p_B} ----------- (i)$\n",
    "\n",
    "conversely, $q_B = \\frac{p_B}{p_A + p_B} ----------- (ii)$\n",
    "\n",
    "Therefore, $Q_min (A) = \\frac{p_A}{p_A + p_B} and Q_min (B) = \\frac{p_B}{p_A + p_B}$\n",
    "\n",
    "*The distribution $Q_min$ is the one that makes $Q(A)$ as close to $P(A)$ as possible, while still being a valid probability distribution. It essentially redistributes the probability mass of P evenly between events A and B.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ii) For a given P, show that there is no upper bound for $D_{KL}(P|Q)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER**\n",
    "\n",
    "Consider discrete distributions $P$ and $Q$ over a finite set of events;\n",
    "\n",
    "suppose there exists an upper bound $M$ such that $D_{KL}(P||Q) \\leq M$ for all distributions $P$ and $Q$.\n",
    "\n",
    "**Consider scenario**: $P(x) > 0$ but $Q(x) = 0$\n",
    "\n",
    "In this case, $P(x \\frac{P(x)}{Q(x)})$ is undefined, hence $D_{KL}(P||Q)$ will be infinite.\n",
    "\n",
    "This proves that there is no upper bound for $D_{KL}(P||Q)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) What is the relationship between KL divergence and cross-entropy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER**\n",
    "\n",
    "============================================================================================================================================================================================================================\n",
    "\n",
    "From a definitional standpoint, ***KL divergence** $(D_KL(P||Q))$ measures the difference between two probability distributions $P$ and $Q$, whereas **cross-entropy** ($- \\sum_{y}P^*(y|x_i)logP(y|x_i;\\theta)$) measures the average number of bits needed to encode events from one probability distribution $(P)$ using the optimal code for another probability distribution $(Q)$.*\n",
    "\n",
    "============================================================================================================================================================================================================================\n",
    "\n",
    "Consider a classification problem where a model returns the probability of an input belonging to a given class, i.e. $P(y|x_i;\\theta)$ = predicted class distribution;\n",
    "\n",
    "Assuming you have the labels of the true class distribution, i.e. $P^*(y|x_i)$, we can use the KL divergence (as shown below) to optimze the model parameters $\\theta$ so that the predicted class distribution is aas close as possible to the underlying true class distribution.\n",
    "\n",
    "$\\operatorname{argmin}_{\\theta} D_KL (P^*||P) = \\operatorname{argmin}_{\\theta} D_KL(P^*(y|x_i)||P(y|x_i;\\theta))$\n",
    "\n",
    "$= \\operatorname{argmin}_{\\theta} \\sum_{y}p^*(y|x_i)log\\frac{P*(y|x_i)}{P(y|x_i;\\theta)}$\n",
    "\n",
    "$= \\operatorname{argmin}_{\\theta} \\sum_{y}P^*(y|x_i)[logP^*(y|x_i) - logP(y|x_i;\\theta)]$\n",
    "\n",
    "$= \\operatorname{argmin}_{\\theta} \\sum_{y}P^*(y|x_i)logP^*(y|x_i) - \\sum_{y}P^*(y|x_i)logP(y|x_i;\\theta)$\n",
    "\n",
    "But since \\sum_{y}P^*(y|x_i)logP^*(y|x_i) does not depend on $\\theta$;\n",
    "\n",
    "$\\operatorname{argmin}_{\\theta} D_KL (P^*||P) \\equiv - \\sum_{y}P^*(y|x_i)logP(y|x_i;\\theta) \\equiv$ to minimizing the cross-entropy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Feature Compression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume you want to perform classification with two classes, $A$ and $B$ in the feature space ${\\rm I\\!R}^{2n}$. We can assume that the two classes follow a normal distribution, with $\\mu_A = (\\mu_1, \\mu_2)$ and $\\mu_B = (\\mu_1, \\mu_3)$, with $\\mu_1, \\mu_2, \\mu_3 \\in {\\rm I\\!R}^n$. $\\Sigma$ is identical for both distributions, see below ($\\sigma \\in [0,1]$, $\\alpha \\approx 1$). You perform a Principal Component Analysis (PCA) for feature space transformation.\n",
    "\n",
    "$$\\sum =\n",
    "\\left(\n",
    "  \\begin{array}{ccc}\n",
    "  \\begin{array}{cc} \n",
    "\\sigma & \\alpha\\\\\n",
    "\\alpha & \\sigma\n",
    "\\end{array} & \\dots & 0  \\\\\n",
    "  \\vdots & \\ddots & \\vdots  \\\\\n",
    "  0 & \\dots & \\begin{array}{cc} \n",
    "\\sigma & \\alpha\\\\\n",
    "\\alpha & \\sigma\n",
    "\\end{array} \n",
    "\\end{array} \\right) \\in \\mathbb{R}^{2n \\times 2n} $$\n",
    "\n",
    "a) Without calculating the result, make a prediction about how the sorted sequence of Eigenvalues will look like! You do not need to give exact numbers, but sketch the graph of Eigenvalues by Eigenvector index. How many components do you anticipate to keep to retain most of the variance in the data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint: you may choose to implement and plot an example for this task.\n",
    "If so, np.random.multivariate_normal and sklearn.decomposition might come in handy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Is the number of features you answered for part a) representative of the minimum number of features required for discriminating the two classes? Justify your answer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3: Logistic Regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression is a simple, but important classification technique (despite the name, it is not used for regression) for binary classification tasks. \n",
    "\n",
    "To classify a sample $x$, we:\n",
    "\n",
    "1. Calculate $z(x) = \\theta^Tx$ (to include a bias term, add a constant feature 1 to $x$).\n",
    "2. Apply $h(x)=\\sigma(z(x))$ with $\\sigma(s)=\\frac{1}{1+e^{-s}}$  \n",
    "3. Apply a threshold $t$ to $h(x)$ to discriminate between the two classes (i.e., assign class 0 to $x \\iff h(x) < t$)\n",
    "\n",
    "For training, we initialize $\\theta$ randomly and perform gradient descent, i.e., loop over the following steps:\n",
    "\n",
    "1. Calculate the loss $J(\\theta)$ on the training data with $J(\\theta) = -y_1 \\cdot log(p_1) - (1-y_1) \\cdot log(1-p_1)$\n",
    "2. Adjust the weights $\\theta$ in the direction of $\\frac{\\delta J}{\\delta \\theta}$ with a learning rate of $l$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Argue why logistic regression can be considered a special case of a neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "**ANSWER**\n",
    "\n",
    "Logistic regression is a special case of a shallow neural network (with no hidden layers) where the activation function is an exponential function, i.e. a \"network\" of only one neuron and its activation function is an exponential function and it maximise the log-likelihood, then its performing logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Assume the logistic regression to detect a target class among non-targets. Describe how you can adjust the algorithm depending on whether a high recall or a high precision are more important in your application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER**\n",
    "\n",
    "**High Recall, $HR = \\frac{\\text{true positives}}{\\text{true positives } + \\text{ false negatives}}$**\n",
    "\n",
    "In case a high recall is required, this will necessitate minimizing the false negatives, i.e. correctly identifying as many instances of the target class as possible, even at the expense of potentially higher false positives. The following adjustments can be made to the algorithm to achieve this;\n",
    "1. Lowering the threshold *t* for classification as the target class, which increases the likelihood of classifying more instances as positive.\n",
    "2. Also, assigning a higher weight to the target class could prioritize the correct classification of target instances.\n",
    "\n",
    "**High Precision, $HP = \\frac{\\text{true positives}}{\\text{true positives } + \\text{ false positives}}$**\n",
    "\n",
    "In case a high precision is required, this will necessitate miinimizing the false positives, i.e. correctly identifying instances as the target class with high confidence, even if it means potentially missing some true positives. In this case, the following adjustments can be made to the algorithm to achieve this;\n",
    "1. Raising the threshold *t* for classification as the target class, which will decrease the number of instances classified as positive\n",
    "2. Also, intuitevely engineering features that are highly indicative of the target class, making it easier for the model to distinguish between the target class and non-target classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Program a classifier object LogisticRegression with methods fit(X,y) and predict(X) that implements training and classification as described above. While you should use PyTorch in all following programming tasks, use only elementary Python and numpy methods. For this purpose, you will need to determine the partial derivative of $J(\\theta)$. Fill out the following skeleton class for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "EPS = 1e-12\n",
    "\n",
    "class MyLogisticRegression:\n",
    "    def __init__(self, lr=0.01, num_iter=100000, verbose=False):\n",
    "        \"\"\"\n",
    "        Initialize logistic regression model parameters.\n",
    "\n",
    "        Args:\n",
    "        - lr: learning rate for gradient descent\n",
    "        - num_iter: number of iterations for gradient descent\n",
    "        - verbose: whether to print loss during training (after every 10000 iterations)\n",
    "        \"\"\"\n",
    "        self.lr = lr\n",
    "        self.num_iter = num_iter\n",
    "        self.verbose = verbose\n",
    "        # added\n",
    "        self.weights = None\n",
    "        self.loss_history = [] \n",
    "    \n",
    "    def __add_intercept(self, X):\n",
    "        \"\"\"\n",
    "        Add intercept term to input features.\n",
    "\n",
    "        Args:\n",
    "        - X: input features\n",
    "\n",
    "        Returns:\n",
    "        - X with intercept term added\n",
    "        \"\"\"\n",
    "        intercept = np.ones((X.shape[0], 1))\n",
    "        return np.concatenate((intercept, X), axis=1)\n",
    "    \n",
    "    def __sigmoid(self, z):\n",
    "        \"\"\"\n",
    "        Compute sigmoid function.\n",
    "\n",
    "        Args:\n",
    "        - z: linear combination of input features and weights\n",
    "\n",
    "        Returns:\n",
    "        - sigmoid(z)\n",
    "        \"\"\"\n",
    "        z = h = np.clip(z, EPS, 1-EPS)\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    def __loss(self, h, y):\n",
    "        \"\"\"\n",
    "        Compute binary cross-entropy loss.\n",
    "\n",
    "        Args:\n",
    "        - h: predicted probabilities\n",
    "        - y: true labels\n",
    "\n",
    "        Returns:\n",
    "        - binary cross-entropy loss\n",
    "        \"\"\"\n",
    "        h = np.clip(h, EPS, 1-EPS)\n",
    "        return (-y * np.log(h) - (1 - y) * np.log(1 - h)).mean()\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        # implement\n",
    "        \"\"\"\n",
    "        Fit logistic regression model using gradient descent.\n",
    "\n",
    "        Args:\n",
    "        - X: input features\n",
    "        - y: true labels\n",
    "        \"\"\"\n",
    "        X = self.__add_intercept(X)\n",
    "        self.weights = np.zeros(X.shape[1])\n",
    "        \n",
    "        for i in range(self.num_iter):\n",
    "            z = np.dot(X, self.weights)\n",
    "            h = self.__sigmoid(z)\n",
    "            gradient = np.dot(X.T, (h - y)) / y.size\n",
    "            self.weights -= self.lr * gradient\n",
    "            \n",
    "            # Calculate and store the loss\n",
    "            loss = self.__loss(h, y)\n",
    "            self.loss_history.append(loss)\n",
    "            \n",
    "            if self.verbose and i % 10000 == 0: # will never run in our study case\n",
    "                # z = np.dot(X, self.weights)\n",
    "                # h = self.__sigmoid(z)\n",
    "                # loss = self.__loss(h, y)\n",
    "                print(f'Iteration {i}, Loss: {loss}')\n",
    "     \n",
    "    def predict_prob(self, X):\n",
    "        # implement\n",
    "        \"\"\"\n",
    "        Predict probabilities of target class.\n",
    "\n",
    "        Args:\n",
    "        - X: input features\n",
    "\n",
    "        Returns:\n",
    "        - predicted probabilities\n",
    "        \"\"\"\n",
    "        X = self.__add_intercept(X)\n",
    "        return self.__sigmoid(np.dot(X, self.weights))\n",
    "    \n",
    "    def predict(self, X, threshold):\n",
    "        # implement\n",
    "        \"\"\"\n",
    "        Predict binary class labels.\n",
    "\n",
    "        Args:\n",
    "        - X: input features\n",
    "        - threshold: threshold for binary classification\n",
    "\n",
    "        Returns:\n",
    "        - predicted binary class labels\n",
    "        \"\"\"\n",
    "        self.predict_prob(X) >= threshold # will predict binary class labels based on threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) Evaluate your logistic regression classifier with the BreastCancer data set (available in scikit-learn). The optimization problem during training of logistic regression is convex, i.e., it will always converge towards a global minimum. How can you verify this empirically?\n",
    "\n",
    "Hint: if you had trouble implementing the logistic regression earlier, you may use the sklearn version here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "(X,y) = load_breast_cancer(return_X_y=True)\n",
    "# implement\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "my_lr_model = MyLogisticRegression()\n",
    "\n",
    "my_lr_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(my_lr_model.loss_history)\n",
    "plt.title('Training Loss History')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()\n",
    "\n",
    "# wrong visual result (its a line, not convex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"1b1\"></div>\n",
    "\n",
    "#### 1) \n",
    "\n",
    "$\\frac{d}{d_(qA)}D_KL(P||Q) = -\\frac{p_A}{q_A} + \\frac{p_B}{1- q_A}$  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# still on paper - todo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
