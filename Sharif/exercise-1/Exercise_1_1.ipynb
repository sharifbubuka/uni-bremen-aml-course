{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditional Probabilities\n",
    "You know the University of Bremen has 18,631 students, of which 6,671 are in natural sciences and engineering (see https://www.uni-bremen.de/en/university/profile/facts-figures ). Three-quarters of your friends in the natural sciences like mate (a beverage) from your personal experience. \n",
    "You are curious if you can determine how likely someone studies in this field, given they like mate. Therefore, you conduct a quick experiment in the mensa and ask at random tables the field and how much they like mate. \n",
    "\n",
    "The following matrix describes your data. The first column describes if the person studies natural sciences (or not) and the second how much they like mate (scale from -2 to 2, higher=likes better, neutral is not allowed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  1],\n",
       "       [ 0, -1],\n",
       "       [ 0,  1],\n",
       "       [ 0, -1],\n",
       "       [ 1,  1],\n",
       "       [ 0,  1],\n",
       "       [ 0, -2],\n",
       "       [ 0, -1]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questionaire_mate = np.array([[True, 1], [False, -1], [False, 1], [False, -1], [True, 1], [False, 1], [False, -2], [False, -1]])\n",
    "questionaire_mate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a person likes mate, how likely are they to study in the natural sciences?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability that a person studies natural sciences, P(A): 0.3580591487306103\n",
      "Probability that a person likes mate, P(B): 0.5\n",
      "Number of students who study natural sciences, num_A:  2\n",
      "Number of students who like mate given they study natural sciences, num_B_given_A:  2\n",
      "Probability that a person likes mate given they study natural sciences, P_B_given_A: 1.0\n",
      "Probability of studying in the natural sciences given that the person likes mate, P_A_given_B: 0.7161182974612206\n"
     ]
    }
   ],
   "source": [
    "# P(A) = Probability that a person studies natural sciences\n",
    "P_A = 6671/18631\n",
    "print(\"Probability that a person studies natural sciences, P(A):\", P_A)\n",
    "\n",
    "#P (B) = Probability that a person likes mate\n",
    "P_B = np.sum(questionaire_mate[:, 1] > 0) / questionaire_mate.shape[0] \n",
    "print(\"Probability that a person likes mate, P(B):\", P_B)\n",
    "\n",
    "# P(A|B) = Probability that a person studies natural sciences, given they like mate\n",
    "# Using Bayes Theorem; P(A|B) = (P(B|A)* P(A)) /P(B)\n",
    "\n",
    "# We have P(A) and P(B), lets find P(B|A) and substitute it in our formular\n",
    "natural_science_students = questionaire_mate[questionaire_mate[:, 0] == 1]\n",
    "print(\"Number of students who study natural sciences, num_A: \", natural_science_students.shape[0])\n",
    "\n",
    "num_like_mate_given_study_natural_sciences = np.sum(natural_science_students[:, 1] > 0)\n",
    "print(\"Number of students who like mate given they study natural sciences, num_B_given_A: \", num_like_mate_given_study_natural_sciences)\n",
    "\n",
    "# Therefore, probability that person likes mate given they study natural sciences, P(B|A);\n",
    "P_B_given_A = num_like_mate_given_study_natural_sciences / natural_science_students.shape[0]\n",
    "print(\"Probability that a person likes mate given they study natural sciences, P_B_given_A:\", P_B_given_A)\n",
    "\n",
    "# Hence;\n",
    "P_A_given_B = (P_B_given_A * P_A) / P_B\n",
    "\n",
    "print(\"Probability of studying in the natural sciences given that the person likes mate, P_A_given_B:\", P_A_given_B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximum Likelihood Estimation\n",
    "A Gaussian normal distribution can be fitted by applying the Maximum Likelihood Estimation to determine the best parameters for explaining a given dataset. This is equivalent to calculating the mean (and variance) on the dataset directly; why?\n",
    "\n",
    "The Gaussian normal distribution is given as follows:\n",
    "\n",
    "$$\n",
    "N(\\mu,\\sigma) = \\frac{1}{\\sigma \\sqrt{2\\pi}} \n",
    "e^{-\\frac{1}{2}(\\frac{x-\\mu}{\\sigma})^2}\n",
    "$$\n",
    "\n",
    "Hint: The partial derivative is easier to compute when using a log likelihood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Kullback-Leibler Divergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$D_{KL}(P|Q) = \\sum_x P(x)log(\\frac{P(x)}{Q(x)})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Calculate the KL divergence for two discrete distributions $P$ and $Q$ over events $A,B,C$. \n",
    "Calculate $D_{KL}(P|Q)$ and $D_{KL}(Q|P)$ and compare! \n",
    "\n",
    "| Distribution | A | B | C |\n",
    "| --- | --- | --- | --- |\n",
    "| P | 0.5 | 0.3 | 0.2 |\n",
    "| Q | 0.4 | 0.2 | 0.4 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KL(P|Q) 0.0945818719775651\n",
      "KL(Q|P) 0.10690843007666134\n"
     ]
    }
   ],
   "source": [
    "p = [0.5,0.3,0.2]\n",
    "q = [0.4,0.2,0.4]\n",
    "\n",
    "# implement or calculate:\n",
    "\n",
    "def KL(a, b):\n",
    "    a = np.asarray(a)\n",
    "    b = np.asarray(b)\n",
    "    \n",
    "    if len(a) != len(b):\n",
    "        raise ValueError(\"Arrays must have the same length.\")\n",
    "    \n",
    "    return np.sum(np.where(a != 0, a * np.log(a/b), 0))\n",
    "\n",
    "print(\"KL(P|Q)\", KL(p, q))\n",
    "print(\"KL(Q|P)\", KL(q, p))\n",
    "\n",
    "# Inconsistent results with kl_div from scipy.special"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$D_{KL}(P|Q)$ is less than $D_{KL}(Q|P)$, indicating that the divergence from Q to P is higher than the divergence from P to Q."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) For this task, assume for simplicity that $P$ and $Q$ are discrete distributions over two events $A,B$. \n",
    "\n",
    "i) For a given $P$, what $Q_{min}$ minimizes $D_{KL}(P|Q)$? Justify your answer!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "**ANSWER**\n",
    "\n",
    "Consider the following; $p_A$ = $P(A)$, $p_B$ = $P(B)$, $q_A$ = $Q(A)$ and $q(B)$ = $Q(B)$\n",
    "\n",
    "Given that we are only considering two events; A and B, therefore, the following constraint holds; $q_A + q_B = 1$\n",
    "\n",
    "To get the value of $q_A$ that minimizes the $D_KL(P|Q)$, set the derivative of the $D_KL(P|Q)$ with respect to $q_A$ to 0, and solve for $q_A$.\n",
    "\n",
    "$\\frac{d}{d_(qA)}D_KL(P||Q) = \\frac{d}{d_(qA)}[p_A log(\\frac{p_A}{q_A}) + p_B log(\\frac{p_B}{q_B})]$\n",
    "\n",
    "Using the chain rule:\n",
    "\n",
    "$\\frac{d}{d_(qA)}D_KL(P||Q) = -\\frac{p_A}{q_A} + \\frac{p_B}{1- q_A}$ (See calculation <a href=\"#1b1\">here</a>.)\n",
    "\n",
    "Setting this to zero, and solving for $q_A$;\n",
    "\n",
    "$-\\frac{p_A}{q_A} + \\frac{p_B}{1- q_A} = 0$\n",
    "\n",
    "$-p_A(1 - q_A) + p_B q_A = 0$\n",
    "\n",
    "$-p_A + p_A q_A + p_Bq_A = 0$\n",
    "\n",
    "$(p_A + p_B)qA = p_A$\n",
    "\n",
    "$q_A = \\frac{p_A}{p_A + p_B} ----------- (i)$\n",
    "\n",
    "conversely, $q_B = \\frac{p_B}{p_A + p_B} ----------- (ii)$\n",
    "\n",
    "Therefore, $Q_min (A) = \\frac{p_A}{p_A + p_B} and Q_min (B) = \\frac{p_B}{p_A + p_B}$\n",
    "\n",
    "*The distribution $Q_min$ is the one that makes $Q(A)$ as close to $P(A)$ as possible, while still being a valid probability distribution. It essentially redistributes the probability mass of P evenly between events A and B.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ii) For a given P, show that there is no upper bound for $D_{KL}(P|Q)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER**\n",
    "\n",
    "Consider discrete distributions $P$ and $Q$ over a finite set of events;\n",
    "\n",
    "suppose there exists an upper bound $M$ such that $D_{KL}(P||Q) \\leq M$ for all distributions $P$ and $Q$.\n",
    "\n",
    "**Consider scenario**: $P(x) > 0$ but $Q(x) = 0$\n",
    "\n",
    "In this case, $P(x \\frac{P(x)}{Q(x)})$ is undefined, hence $D_{KL}(P||Q)$ will be infinite.\n",
    "\n",
    "This proves that there is no upper bound for $D_{KL}(P||Q)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) What is the relationship between KL divergence and cross-entropy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER**\n",
    "\n",
    "============================================================================================================================================================================================================================\n",
    "\n",
    "From a definitional standpoint, ***KL divergence** $(D_KL(P||Q))$ measures the difference between two probability distributions $P$ and $Q$, whereas **cross-entropy** ($- \\sum_{y}P^*(y|x_i)logP(y|x_i;\\theta)$) measures the average number of bits needed to encode events from one probability distribution $(P)$ using the optimal code for another probability distribution $(Q)$.*\n",
    "\n",
    "============================================================================================================================================================================================================================\n",
    "\n",
    "Consider a classification problem where a model returns the probability of an input belonging to a given class, i.e. $P(y|x_i;\\theta)$ = predicted class distribution;\n",
    "\n",
    "Assuming you have the labels of the true class distribution, i.e. $P^*(y|x_i)$, we can use the KL divergence (as shown below) to optimze the model parameters $\\theta$ so that the predicted class distribution is aas close as possible to the underlying true class distribution.\n",
    "\n",
    "$\\operatorname{argmin}_{\\theta} D_KL (P^*||P) = \\operatorname{argmin}_{\\theta} D_KL(P^*(y|x_i)||P(y|x_i;\\theta))$\n",
    "\n",
    "$= \\operatorname{argmin}_{\\theta} \\sum_{y}p^*(y|x_i)log\\frac{P*(y|x_i)}{P(y|x_i;\\theta)}$\n",
    "\n",
    "$= \\operatorname{argmin}_{\\theta} \\sum_{y}P^*(y|x_i)[logP^*(y|x_i) - logP(y|x_i;\\theta)]$\n",
    "\n",
    "$= \\operatorname{argmin}_{\\theta} \\sum_{y}P^*(y|x_i)logP^*(y|x_i) - \\sum_{y}P^*(y|x_i)logP(y|x_i;\\theta)$\n",
    "\n",
    "But since \\sum_{y}P^*(y|x_i)logP^*(y|x_i) does not depend on $\\theta$;\n",
    "\n",
    "$\\operatorname{argmin}_{\\theta} D_KL (P^*||P) \\equiv - \\sum_{y}P^*(y|x_i)logP(y|x_i;\\theta) \\equiv$ to minimizing the cross-entropy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Feature Compression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume you want to perform classification with two classes, $A$ and $B$ in the feature space ${\\rm I\\!R}^{2n}$. We can assume that the two classes follow a normal distribution, with $\\mu_A = (\\mu_1, \\mu_2)$ and $\\mu_B = (\\mu_1, \\mu_3)$, with $\\mu_1, \\mu_2, \\mu_3 \\in {\\rm I\\!R}^n$. $\\Sigma$ is identical for both distributions, see below ($\\sigma \\in [0,1]$, $\\alpha \\approx 1$). You perform a Principal Component Analysis (PCA) for feature space transformation.\n",
    "\n",
    "$$\\sum =\n",
    "\\left(\n",
    "  \\begin{array}{ccc}\n",
    "  \\begin{array}{cc} \n",
    "\\sigma & \\alpha\\\\\n",
    "\\alpha & \\sigma\n",
    "\\end{array} & \\dots & 0  \\\\\n",
    "  \\vdots & \\ddots & \\vdots  \\\\\n",
    "  0 & \\dots & \\begin{array}{cc} \n",
    "\\sigma & \\alpha\\\\\n",
    "\\alpha & \\sigma\n",
    "\\end{array} \n",
    "\\end{array} \\right) \\in \\mathbb{R}^{2n \\times 2n} $$\n",
    "\n",
    "a) Without calculating the result, make a prediction about how the sorted sequence of Eigenvalues will look like! You do not need to give exact numbers, but sketch the graph of Eigenvalues by Eigenvector index. How many components do you anticipate to keep to retain most of the variance in the data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint: you may choose to implement and plot an example for this task.\n",
    "If so, np.random.multivariate_normal and sklearn.decomposition might come in handy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Is the number of features you answered for part a) representative of the minimum number of features required for discriminating the two classes? Justify your answer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3: Logistic Regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression is a simple, but important classification technique (despite the name, it is not used for regression) for binary classification tasks. \n",
    "\n",
    "To classify a sample $x$, we:\n",
    "\n",
    "1. Calculate $z(x) = \\theta^Tx$ (to include a bias term, add a constant feature 1 to $x$).\n",
    "2. Apply $h(x)=\\sigma(z(x))$ with $\\sigma(s)=\\frac{1}{1+e^{-s}}$  \n",
    "3. Apply a threshold $t$ to $h(x)$ to discriminate between the two classes (i.e., assign class 0 to $x \\iff h(x) < t$)\n",
    "\n",
    "For training, we initialize $\\theta$ randomly and perform gradient descent, i.e., loop over the following steps:\n",
    "\n",
    "1. Calculate the loss $J(\\theta)$ on the training data with $J(\\theta) = -y_1 \\cdot log(p_1) - (1-y_1) \\cdot log(1-p_1)$\n",
    "2. Adjust the weights $\\theta$ in the direction of $\\frac{\\delta J}{\\delta \\theta}$ with a learning rate of $l$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Argue why logistic regression can be considered a special case of a neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Assume the logistic regression to detect a target class among non-targets. Describe how you can adjust the algorithm depending on whether a high recall or a high precision are more important in your application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Program a classifier object LogisticRegression with methods fit(X,y) and predict(X) that implements training and classification as described above. While you should use PyTorch in all following programming tasks, use only elementary Python and numpy methods. For this purpose, you will need to determine the partial derivative of $J(\\theta)$. Fill out the following skeleton class for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPS = 1e-12\n",
    "\n",
    "class MyLogisticRegression:\n",
    "    def __init__(self, lr=0.01, num_iter=100000, verbose=False):\n",
    "        self.lr = lr\n",
    "        self.num_iter = num_iter\n",
    "        self.verbose = verbose\n",
    "    \n",
    "    def __add_intercept(self, X):\n",
    "        intercept = np.ones((X.shape[0], 1))\n",
    "        return np.concatenate((intercept, X), axis=1)\n",
    "    \n",
    "    def __sigmoid(self, z):\n",
    "        z = h = np.clip(z, EPS, 1-EPS)\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    def __loss(self, h, y):\n",
    "        h = np.clip(h, EPS, 1-EPS)\n",
    "        return (-y * np.log(h) - (1 - y) * np.log(1 - h)).mean()\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        # implement\n",
    "        pass\n",
    "     \n",
    "    def predict_prob(self, X):\n",
    "        # implement\n",
    "        pass\n",
    "    \n",
    "    def predict(self, X, threshold):\n",
    "        # implement\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) Evaluate your logistic regression classifier with the BreastCancer data set (available in scikit-learn). The optimization problem during training of logistic regression is convex, i.e., it will always converge towards a global minimum. How can you verify this empirically?\n",
    "\n",
    "Hint: if you had trouble implementing the logistic regression earlier, you may use the sklearn version here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "(X,y) = load_breast_cancer(return_X_y=True)\n",
    "# implement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"1b1\"></div>\n",
    "\n",
    "#### 1) \n",
    "\n",
    "$\\frac{d}{d_(qA)}D_KL(P||Q) = -\\frac{p_A}{q_A} + \\frac{p_B}{1- q_A}$  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# still on paper - todo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
