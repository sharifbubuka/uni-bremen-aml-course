{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ceb586da",
   "metadata": {},
   "source": [
    "# Task 4\n",
    "In this exercise, you will implement a model to classify images. Each image belongs to one out of ten classes and is given as RGB image with 32x32 pixels. This task focuses on the creation of data lists to load your data and on the implementation of a custom dataset class.\n",
    "\n",
    "**a)** Download the dataset to your hard disk and extract the files. The data is provided as train-test split and the labels of the respective images are given by the name of the parent folder. Download the CIFAR10 dataset from  https://www.kaggle.com/datasets/swaroopkml/cifar10-pngs-in-folders?resource=download or from https://owncloud.csl.uni-bremen.de/s/9mNnmeA7esyEpnC. The corresponding paper of the CIFAR10 dataset: *\"Learning Multiple Layers of Features from Tiny Images\", Alex Krizhevsky, 2009*.\n",
    "\n",
    "**b)** Take the last 20 % of the images of each class and save them in a separate folder structure for validation, e.g.:  \n",
    "*\\user\\data\\cifar10\\validate\\airplane\\4001.png  \n",
    "\\user\\data\\cifar10\\validate\\airplane\\4002.png  \n",
    "...  \n",
    "\\user\\data\\cifar10\\validation\\bird\\4001.png  \n",
    "...*  \n",
    "This leads to the following datasplit for each class: 4,000/1,000/1,000 images for training/validation/testing respectively.\n",
    "\n",
    "**c)** Create one data file list for the training data, one for the validation data, and one for the test data. Each list contains the paths to the different images and the respective labels. You can save the lists as .txt files. You will use these lists in your custom dataset class. One possible example is given in the following.  \n",
    "The training_list.txt contains the respective training data paths and the labels, separated by white spaces:  \n",
    "*\\user\\data\\cifar10\\train\\airplane\\0001.png airplane  \n",
    "\\user\\data\\cifar10\\train\\airplane\\0002.png airplane  \n",
    "...  \n",
    "\\user\\data\\cifar10\\train\\bird\\0001.png bird  \n",
    "...*  \n",
    "\n",
    "The validation_list.txt contains the respective validation data paths and the labels, separated by white spaces.  \n",
    "*\\user\\data\\cifar10\\validate\\airplane\\4001.png airplane  \n",
    "\\user\\data\\cifar10\\validate\\airplane\\4002.png airplane  \n",
    "...  \n",
    "\\user\\data\\cifar10\\validate\\bird\\4001.png bird  \n",
    "...*  \n",
    "\n",
    "The test_list.txt contains the respective test data paths and the labels, separated by white spaces.  \n",
    "*\\user\\data\\cifar10\\test\\airplane\\0001.png airplane  \n",
    "\\user\\data\\cifar10\\test\\airplane\\0002.png airplane  \n",
    "...  \n",
    "\\user\\data\\cifar10\\test\\bird\\0001.png bird  \n",
    "...*  \n",
    "\n",
    "**d)** Write a custom dataset class to define how to load and prepare the data. You can use the PyTorch dataloader class to interface your custom dataset class.  \n",
    "**HINT1:** Use the io method of the scikit-image package to load your data. You need to install the scikit-image package to your AML_Tut conda environment.  \n",
    "**HINT2:** You can install and use the cv2 package (opencv) to display RGB images.\n",
    "\n",
    "**e)** Complete the script to create, train and test a classification model. It is not required to develope a complex model with high classification accuracy. This tasks aims to provide you withe hands-on experience on how to set up, train, and test a machine learning system for a specific task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c15eeaea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced Machine Learning Tutorials\n",
    "# Excercise Sheet 1 - Task 5\n",
    "# Authors: Marvin Borsdorf, Yale Hartmann\n",
    "# Cognitive Systems Lab, University of Bremen, Germany\n",
    "# Last edit: 2021/04/26\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import torch as th\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from skimage import io\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "da1c9c25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\m-gre\\\\Documents\\\\Advanced Machine Learning\\\\uni-bremen-aml-course\\\\Mario\\\\Exercise_1\\\\cifar10\\\\train'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root_dir = r'C:\\Users\\m-gre\\Documents\\Advanced Machine Learning\\uni-bremen-aml-course\\Mario\\Exercise_1'\n",
    "tr_dir = os.path.join(root_dir, 'cifar10', 'train')\n",
    "cv_dir = os.path.join(root_dir, 'cifar10', 'validate')  \n",
    "tt_dir = os.path.join(root_dir, 'cifar10', 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6fdc06de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datalist from directory for each subset\n",
    "tr_file_list = []\n",
    "cv_file_list = []\n",
    "tt_file_list = []\n",
    "\n",
    "# Training\n",
    "for path, subdirs, files in os.walk(tr_dir):\n",
    "    tr_file_list.append([os.path.join(path, s) + \" \" + os.path.basename(path) for s in files])\n",
    "        \n",
    "# Validation\n",
    "for path, subdirs, files in os.walk(cv_dir):\n",
    "    tr_file_list.append([os.path.join(path, s) + \" \" + os.path.basename(path) for s in files])\n",
    "\n",
    "# Test\n",
    "for path, subdirs, files in os.walk(tt_dir):\n",
    "    tr_file_list.append([os.path.join(path, s) + \" \" + os.path.basename(path) for s in files])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e4c53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define custom dataset\n",
    "class cifar10Dataset(Dataset):\n",
    "    def __init__(self): \n",
    "        # You need access to the data file lists. Provide the list as parameter somehow...\n",
    "        # Create the final splitted data and label lists\n",
    "        # ...complete code here\n",
    "        self._init_data()\n",
    "        \n",
    "    def __len__(self):                      # Get total number of dataset samples\n",
    "        # ...complete code here\n",
    "\n",
    "    def __getitem__(self, idx):             # Return an item\n",
    "        # ...complete code here\n",
    "        return data, label\n",
    "        \n",
    "    def _init_data(self):                   # Initialize data\n",
    "        # ...complete code here\n",
    "        # Read file\n",
    "        # This is also a good place to encode your string labels into integer labels for the model prediction (-> use a LabelEncoder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53191a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model (neural network)\n",
    "class myNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        # ...complete code here\n",
    "\n",
    "    def forward(self, x):\n",
    "        # ...complete code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb9a1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss\n",
    "criterion = nn.CrossEntropyLoss()  # Applies log_softmax and negative log likelihood loss\n",
    "\n",
    "# Create datasets\n",
    "training_dataset = # ...complete code here\n",
    "validation_dataset = # ...complete code here\n",
    "\n",
    "# PyTorch dataloader\n",
    "training_loader = # ...complete code here\n",
    "validation_loader = # ...complete code here\n",
    "\n",
    "# Create model\n",
    "model = # ...complete code here\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.6) # You may change the optimizer, the learning rate, and other hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf28a50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training step\n",
    "def training(model, data_loader, optimizer):\n",
    "    training_losses = []\n",
    "    training_correct = []\n",
    "    \n",
    "    model.train()                   # Important\n",
    "    for data in tqdm(data_loader):\n",
    "        inputs, labels = data\n",
    "        \n",
    "        # ...complete code here\n",
    "        # Zero the last gradients\n",
    "        # Do prediction\n",
    "        \n",
    "        labels = labels.long()\n",
    "        \n",
    "        # ...complete code here\n",
    "        # Calculate loss\n",
    "        # Do backpropagation and calculate gradients\n",
    "        # Update weights\n",
    "        \n",
    "        training_losses.append(loss.item())\n",
    "        correct = (th.argmax(predictions, dim=1) == labels) # Check for correct classification\n",
    "        training_correct.append(sum(correct).item())\n",
    "    \n",
    "    training_accuracy = 100*(np.sum(training_correct)/len(data_loader.dataset))\n",
    "    return np.mean(training_losses), training_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204d5dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define validation step\n",
    "def validation(model, data_loader, optimizer):\n",
    "    validation_losses = []\n",
    "    validation_correct = []\n",
    "    \n",
    "    model.eval()                        # Important to disable parts such as dropout, batch norm, etc.\n",
    "    with th.no_grad():                  # No gradients\n",
    "        for data in tqdm(data_loader):\n",
    "            inputs, labels = data\n",
    "            \n",
    "            # ...complete code here\n",
    "            # Do prediction\n",
    "            \n",
    "            labels = labels.long()\n",
    "            \n",
    "            # ...complete code here\n",
    "            # Calculate loss\n",
    "            \n",
    "            validation_losses.append(loss.item())\n",
    "            correct = (th.argmax(predictions, dim=1) == labels) # Check for correct classification\n",
    "            validation_correct.append(sum(correct).item())\n",
    "    \n",
    "    validation_accuracy = 100*(np.sum(validation_correct)/len(data_loader.dataset))\n",
    "    return np.mean(validation_losses), validation_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e474b1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the training and the validation\n",
    "epochs = 1\n",
    "print(\"Train on %d samples.\" % len(training_loader.dataset))\n",
    "print(\"Validate on %d samples.\" % len(validation_loader.dataset))\n",
    "\n",
    "for e in range(epochs):\n",
    "    print(\"###### Epoch %d ######\" %(e+1))\n",
    "    tr_loss, tr_acc = training(model, training_loader, optimizer)\n",
    "    print(\"Training loss: %.5f. Training accuracy: %.2f %%.\" % (tr_loss, tr_acc))\n",
    "    val_loss, val_acc = validation(model, validation_loader, optimizer)\n",
    "    print(\"Validation loss: %.5f. Validation accuracy: %.2f %%.\" % (val_loss, val_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcda8806",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model and optimizer state dictionaries\n",
    "th.save(model.state_dict(), os.path.join(root_dir, 'model_state_dict.pt'))   \n",
    "th.save(optimizer.state_dict(), os.path.join(root_dir, 'optimizer_state_dict.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ecd626",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new model and load pre-trained parameters\n",
    "# ...complete code here\n",
    "model2.load_state_dict(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff92a560",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test both models on test input\n",
    "model.eval()    # Important to disable parts such as dropout, batch norm, etc.\n",
    "model2.eval()   \n",
    "test_dataset = # ...complete code here\n",
    "test_loader = # ...complete code here\n",
    "m1_losses = []\n",
    "m1_correct = []\n",
    "m2_losses = []\n",
    "m2_correct = []\n",
    "\n",
    "print(\"Test on %d samples.\" % len(test_loader.dataset))\n",
    "\n",
    "for data in tqdm(test_loader):\n",
    "        _input, label = data\n",
    "        label = label.long()\n",
    "\n",
    "        # For model 1\n",
    "        \n",
    "        # ...complete code here\n",
    "        # Do prediction\n",
    "        # Calculate loss\n",
    "        \n",
    "        m1_losses.append(loss.item())\n",
    "        correct = (th.argmax(pred1, dim=1) == label) # Check for correct classification\n",
    "        m1_correct.append(sum(correct).item())\n",
    "        \n",
    "        \n",
    "        # For model 2\n",
    "        \n",
    "        # ...complete code here\n",
    "        # Do prediction\n",
    "        # Calculate loss\n",
    "        \n",
    "        m2_losses.append(loss.item())\n",
    "        correct = (th.argmax(pred2, dim=1) == label) # Check for correct classification\n",
    "        m2_correct.append(sum(correct).item())\n",
    "        \n",
    "        # Uncomment to print image and prediction\n",
    "        #print(\"Model 1 prediction: %d -> %s\" %(th.argmax(pred1, dim=1), test_dataset.label_encoder.inverse_transform(th.argmax(pred1, dim=1))))\n",
    "        #print(\"Model 2 prediction: %d -> %s\" %(th.argmax(pred2, dim=1), test_dataset.label_encoder.inverse_transform(th.argmax(pred2, dim=1))))\n",
    "        #plt.imshow(cv2.cvtColor(th.squeeze(_input).numpy().reshape(32, 32, 3), cv2.COLOR_BGR2RGB))\n",
    "        #plt.show()\n",
    "\n",
    "# Calculate final scores\n",
    "m1_loss = np.mean(m1_losses)\n",
    "m1_acc = 100*(np.sum(m1_correct)/len(test_loader.dataset))\n",
    "m2_loss = np.mean(m2_losses)\n",
    "m2_acc = 100*(np.sum(m2_correct)/len(test_loader.dataset))\n",
    "\n",
    "print(\"Model 1\")\n",
    "print(\"Test loss: %.5f. Test accuracy: %.2f %%.\" % (m1_loss, m1_acc))\n",
    "print(\"Model 2\")\n",
    "print(\"Test loss: %.5f. Test accuracy: %.2f %%.\" % (m2_loss, m2_acc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fffe19d4",
   "metadata": {},
   "source": [
    "Solution could get accuracy of 96%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
