{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditional Probabilities\n",
    "You know the University of Bremen has 18,631 students, of which 6,671 are in natural sciences and engineering (see https://www.uni-bremen.de/en/university/profile/facts-figures ). Three-quarters of your friends in the natural sciences like mate (a beverage) from your personal experience. \n",
    "You are curious if you can determine how likely someone studies in this field, given they like mate. Therefore, you conduct a quick experiment in the mensa and ask at random tables the field and how much they like mate. \n",
    "\n",
    "The following matrix describes your data. The first column describes if the person studies natural sciences (or not) and the second how much they like mate (scale from -2 to 2, higher=likes better, neutral is not allowed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  1],\n",
       "       [ 0, -1],\n",
       "       [ 0,  1],\n",
       "       [ 0, -1],\n",
       "       [ 1,  1],\n",
       "       [ 0,  1],\n",
       "       [ 0, -2],\n",
       "       [ 0, -1]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questionaire_mate = np.array([[True, 1], [False, -1], [False, 1], [False, -1], [True, 1], [False, 1], [False, -2], [False, -1]])\n",
    "questionaire_mate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a person likes mate, how likely are they to study in the natural sciences?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chance that a student studies natural sciences: 0.3580591487306103\n",
      "Chance that student likes mate: 0.5\n",
      "Chance that student that studies natural sciences likes mate: 0.75\n",
      "Chance that student that likes mate studies natural sciences: 0.5370887230959154\n"
     ]
    }
   ],
   "source": [
    "# change second column to boolean to differentiate only between *like* and *dont like*\n",
    "questionaire_mate[:,1] = [True if x > 0 else False for x in questionaire_mate[:,1]]\n",
    "\n",
    "# calculate using the bayes theorem (https://en.wikipedia.org/wiki/Bayes%27_theorem)\n",
    "p_ns = 6671/18631\n",
    "print(f\"Chance that a student studies natural sciences: {p_ns}\")\n",
    "p_mate = sum(questionaire_mate[:,1])/len(questionaire_mate)\n",
    "print(f\"Chance that student likes mate: {p_mate}\")\n",
    "p_ns_mate = 3/4\n",
    "print(f\"Chance that student that studies natural sciences likes mate: {p_ns_mate}\")\n",
    "p_mate_ns = p_ns * p_ns_mate / p_mate\n",
    "print(f\"Chance that student that likes mate studies natural sciences: {p_mate_ns}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximum Likelihood Estimation\n",
    "A Gaussian normal distribution can be fitted by applying the Maximum Likelihood Estimation to determine the best parameters for explaining a given dataset. This is equivalent to calculating the mean (and variance) on the dataset directly; why?\n",
    "\n",
    "The Gaussian normal distribution is given as follows:\n",
    "\n",
    "$$\n",
    "N(\\mu,\\sigma) = \\frac{1}{\\sigma \\sqrt{2\\pi}} \n",
    "e^{-\\frac{1}{2}(\\frac{x-\\mu}{\\sigma})^2}\n",
    "$$\n",
    "\n",
    "Hint: The partial derivative is easier to compute when using a log likelihood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Kullback-Leibler Divergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$D_{KL}(P|Q) = \\sum_x P(x)log(\\frac{P(x)}{Q(x)})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Calculate the KL divergence for two discrete distributions $P$ and $Q$ over events $A,B,C$. \n",
    "Calculate $D_{KL}(P|Q)$ and $D_{KL}(Q|P)$ and compare! \n",
    "\n",
    "| Distribution | A | B | C |\n",
    "| --- | --- | --- | --- |\n",
    "| P | 0.5 | 0.3 | 0.2 |\n",
    "| Q | 0.4 | 0.2 | 0.4 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = [0.5,0.3,0.2]\n",
    "q = [0.4,0.2,0.4]\n",
    "# implement or calculate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) For this task, assume for simplicity that $P$ and $Q$ are discrete distributions over two events $A,B$. \n",
    "\n",
    "i) For a given $P$, what $Q_{min}$ minimizes $D_{KL}(P|Q)$? Justify your answer!\n",
    "\n",
    "ii) For a given P, show that there is no upper bound for $D_{KL}(P|Q)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement or calculate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) What is the relationship between KL divergence and cross-entropy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Feature Compression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume you want to perform classification with two classes, $A$ and $B$ in the feature space ${\\rm I\\!R}^{2n}$. We can assume that the two classes follow a normal distribution, with $\\mu_A = (\\mu_1, \\mu_2)$ and $\\mu_B = (\\mu_1, \\mu_3)$, with $\\mu_1, \\mu_2, \\mu_3 \\in {\\rm I\\!R}^n$. $\\Sigma$ is identical for both distributions, see below ($\\sigma \\in [0,1]$, $\\alpha \\approx 1$). You perform a Principal Component Analysis (PCA) for feature space transformation.\n",
    "\n",
    "$$\\sum =\n",
    "\\left(\n",
    "  \\begin{array}{ccc}\n",
    "  \\begin{array}{cc} \n",
    "\\sigma & \\alpha\\\\\n",
    "\\alpha & \\sigma\n",
    "\\end{array} & \\dots & 0  \\\\\n",
    "  \\vdots & \\ddots & \\vdots  \\\\\n",
    "  0 & \\dots & \\begin{array}{cc} \n",
    "\\sigma & \\alpha\\\\\n",
    "\\alpha & \\sigma\n",
    "\\end{array} \n",
    "\\end{array} \\right) \\in \\mathbb{R}^{2n \\times 2n} $$\n",
    "\n",
    "a) Without calculating the result, make a prediction about how the sorted sequence of Eigenvalues will look like! You do not need to give exact numbers, but sketch the graph of Eigenvalues by Eigenvector index. How many components do you anticipate to keep to retain most of the variance in the data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint: you may choose to implement and plot an example for this task.\n",
    "If so, np.random.multivariate_normal and sklearn.decomposition might come in handy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Is the number of features you answered for part a) representative of the minimum number of features required for discriminating the two classes? Justify your answer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3: Logistic Regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression is a simple, but important classification technique (despite the name, it is not used for regression) for binary classification tasks. \n",
    "\n",
    "To classify a sample $x$, we:\n",
    "\n",
    "1. Calculate $z(x) = \\theta^Tx$ (to include a bias term, add a constant feature 1 to $x$).\n",
    "2. Apply $h(x)=\\sigma(z(x))$ with $\\sigma(s)=\\frac{1}{1+e^{-s}}$  \n",
    "3. Apply a threshold $t$ to $h(x)$ to discriminate between the two classes (i.e., assign class 0 to $x \\iff h(x) < t$)\n",
    "\n",
    "For training, we initialize $\\theta$ randomly and perform gradient descent, i.e., loop over the following steps:\n",
    "\n",
    "1. Calculate the loss $J(\\theta)$ on the training data with $J(\\theta) = -y_1 \\cdot log(p_1) - (1-y_1) \\cdot log(1-p_1)$\n",
    "2. Adjust the weights $\\theta$ in the direction of $\\frac{\\delta J}{\\delta \\theta}$ with a learning rate of $l$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Argue why logistic regression can be considered a special case of a neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Assume the logistic regression to detect a target class among non-targets. Describe how you can adjust the algorithm depending on whether a high recall or a high precision are more important in your application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Program a classifier object LogisticRegression with methods fit(X,y) and predict(X) that implements training and classification as described above. While you should use PyTorch in all following programming tasks, use only elementary Python and numpy methods. For this purpose, you will need to determine the partial derivative of $J(\\theta)$. Fill out the following skeleton class for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPS = 1e-12\n",
    "\n",
    "class MyLogisticRegression:\n",
    "    def __init__(self, lr=0.01, num_iter=100000, verbose=False):\n",
    "        self.lr = lr\n",
    "        self.num_iter = num_iter\n",
    "        self.verbose = verbose\n",
    "    \n",
    "    def __add_intercept(self, X):\n",
    "        intercept = np.ones((X.shape[0], 1))\n",
    "        return np.concatenate((intercept, X), axis=1)\n",
    "    \n",
    "    def __sigmoid(self, z):\n",
    "        z = h = np.clip(z, EPS, 1-EPS)\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    def __loss(self, h, y):\n",
    "        h = np.clip(h, EPS, 1-EPS)\n",
    "        return (-y * np.log(h) - (1 - y) * np.log(1 - h)).mean()\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        # implement\n",
    "        pass\n",
    "     \n",
    "    def predict_prob(self, X):\n",
    "        # implement\n",
    "        pass\n",
    "    \n",
    "    def predict(self, X, threshold):\n",
    "        # implement\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) Evaluate your logistic regression classifier with the BreastCancer data set (available in scikit-learn). The optimization problem during training of logistic regression is convex, i.e., it will always converge towards a global minimum. How can you verify this empirically?\n",
    "\n",
    "Hint: if you had trouble implementing the logistic regression earlier, you may use the sklearn version here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "(X,y) = load_breast_cancer(return_X_y=True)\n",
    "# implement"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
