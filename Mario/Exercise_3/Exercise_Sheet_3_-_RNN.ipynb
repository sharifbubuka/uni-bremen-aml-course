{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to the exercise sheet about Recurrent Neural Networks. In this exercise sheet, we will take a closer look into RNNs, LSTMs and other variations.\n",
    "\n",
    "\n",
    "The main task is to implement the same models as in the lecture and run the classification on the MNIST dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first import all the dependencies we will need for this exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dsets\n",
    "from torch.autograd import Variable\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Dataset and making it iterable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_dataset = dsets.MNIST(root='./data', \n",
    "                            train=True, \n",
    "                            transform=transforms.ToTensor(),\n",
    "                            download=True)\n",
    "\n",
    "test_dataset = dsets.MNIST(root='./data', \n",
    "                           train=False, \n",
    "                           transform=transforms.ToTensor())\n",
    "\n",
    "\n",
    "batch_size = 100\n",
    "n_iters = 3000\n",
    "num_epochs = n_iters / (len(train_dataset) / batch_size)\n",
    "num_epochs = int(num_epochs)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.1: Creating the model classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the RNN and the LSTM models from the lecture starting with one hidden layer and a tanh activation function for the RNN. Hint: The PyTorch packages provides built-in RNN and LSTM models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The RNN\n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, layer_dim, add_fc, output_dim):\n",
    "        super(RNNModel, self).__init__()\n",
    "        # Parameters\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.layer_dim = layer_dim\n",
    "        self.add_fc = add_fc\n",
    "\n",
    "        # Building your RNN\n",
    "        self.rnn = nn.RNN(input_dim, self.hidden_dim, self.layer_dim, batch_first=True, nonlinearity='tanh')\n",
    "       \n",
    "        self.fc = nn.Linear(self.hidden_dim, self.hidden_dim)\n",
    "        # Readout layer\n",
    "        self.ro = nn.Linear(self.hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Initialize hidden state with zeros\n",
    "        h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).to(device)\n",
    "        \n",
    "        #Define the forward steps\n",
    "        out, hn = self.rnn(x, h0)\n",
    "\n",
    "        if self.add_fc:\n",
    "            out = self.fc(out[:, -1, :])\n",
    "            out = torch.tanh(out)\n",
    "            out = self.ro(out)\n",
    "        else:\n",
    "            out = self.ro(out[:, -1, :])\n",
    "\n",
    "        return out\n",
    "    \n",
    "# The LSTM\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, layer_dim, add_fc, output_dim):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        # Parameters\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.layer_dim = layer_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.add_fc = add_fc\n",
    "        \n",
    "        # Building your LSTM\n",
    "        self.lstm = nn.LSTM(self.input_dim, self.hidden_dim, self.layer_dim, batch_first=True)\n",
    "\n",
    "        self.fc = nn.Linear(self.hidden_dim, self.hidden_dim)\n",
    "        \n",
    "        # Readout layer\n",
    "        self.ro = nn.Linear(self.hidden_dim, self.output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Initialize hidden state and cell state with zeros\n",
    "        h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).to(device)\n",
    "\n",
    "        # Define the forward steps\n",
    "        out, (hn, cn) = self.lstm(x, (h0, h0))\n",
    "\n",
    "\n",
    "        if self.add_fc:\n",
    "            out = self.fc(out[:, -1, :])\n",
    "            out = torch.tanh(out)\n",
    "            out = self.ro(out)\n",
    "        else:\n",
    "            out = self.ro(out[:, -1, :])\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.2: Instantiations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instantiate the model classes\n",
    "\n",
    "model_rnn  = RNNModel(input_dim=28, hidden_dim=100, layer_dim=1, output_dim=10, add_fc=False)\n",
    "model_lstm = LSTMModel(input_dim=28, hidden_dim=100, layer_dim=1, output_dim=10, add_fc=False)\n",
    "\n",
    "\n",
    "#Move to GPU if available\n",
    "model_rnn.to(device)\n",
    "model_lstm.to(device)\n",
    "    \n",
    "#Instantiate the Loss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "#Instantiate the Optimizer\n",
    "learning_rate = 0.1\n",
    "optimizer_rnn = torch.optim.SGD(model_rnn.parameters(), lr=learning_rate)\n",
    "optimizer_lstm = torch.optim.SGD(model_lstm.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1.3: Training the models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Below, you find the training steps for the RNN model. Implement the training for the LSTM model accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 500. Loss: 0.4945976138114929. Accuracy: 78.69\n",
      "Iteration: 1000. Loss: 0.49801528453826904. Accuracy: 87.58\n",
      "Iteration: 1500. Loss: 0.35228458046913147. Accuracy: 88.0\n",
      "Iteration: 2000. Loss: 0.45710206031799316. Accuracy: 89.81\n",
      "Iteration: 2500. Loss: 0.30754828453063965. Accuracy: 91.51\n",
      "Iteration: 3000. Loss: 0.11499004065990448. Accuracy: 93.26\n"
     ]
    }
   ],
   "source": [
    "# RNN Training\n",
    "# Number of steps to unroll\n",
    "seq_dim = 28\n",
    "input_dim = 28\n",
    "\n",
    "iter = 0\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # Load images as Variable\n",
    "        images = images.view(-1, seq_dim, input_dim).to(device)\n",
    "        labels = labels.to(device)\n",
    "            \n",
    "        # Clear gradients w.r.t. parameters\n",
    "        optimizer_rnn.zero_grad()\n",
    "        \n",
    "        # Forward pass to get output/logits\n",
    "        outputs = model_rnn(images)\n",
    "        \n",
    "        # Calculate Loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Getting gradients w.r.t. parameters\n",
    "        loss.backward()\n",
    "        \n",
    "        # Updating parameters\n",
    "        optimizer_rnn.step()\n",
    "        \n",
    "        iter += 1\n",
    "        \n",
    "        if iter % 500 == 0:\n",
    "            # Calculate Accuracy         \n",
    "            correct = 0\n",
    "            total = 0\n",
    "            # Iterate through test dataset\n",
    "            for images, labels in test_loader:\n",
    "                images = Variable(images.view(-1, seq_dim, input_dim).to(device))\n",
    "                \n",
    "                # Forward pass only to get logits/output\n",
    "                outputs = model_rnn(images)\n",
    "                \n",
    "                # Get predictions from the maximum value\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                \n",
    "                # Total number of labels\n",
    "                total += labels.size(0)\n",
    "                \n",
    "                # Total correct predictions\n",
    "                correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            accuracy = 100 * correct / total\n",
    "            \n",
    "            # Print Loss\n",
    "            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 500. Loss: 2.2657244205474854. Accuracy: 23.41\n",
      "Iteration: 1000. Loss: 1.1522836685180664. Accuracy: 60.02\n",
      "Iteration: 1500. Loss: 0.37644341588020325. Accuracy: 87.31\n",
      "Iteration: 2000. Loss: 0.3536624014377594. Accuracy: 92.06\n",
      "Iteration: 2500. Loss: 0.20170186460018158. Accuracy: 94.53\n",
      "Iteration: 3000. Loss: 0.29433974623680115. Accuracy: 92.53\n"
     ]
    }
   ],
   "source": [
    "# LSTM Training\n",
    "# Number of steps to unroll\n",
    "seq_dim = 28\n",
    "input_dim = 28\n",
    "\n",
    "iter = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # Load images as Variable\n",
    "        images = images.view(-1, seq_dim, input_dim).to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Clear gradients w.r.t. parameters\n",
    "        optimizer_lstm.zero_grad()\n",
    "        \n",
    "        # Forward pass to get output/logits\n",
    "        outputs = model_lstm(images)\n",
    "        \n",
    "        # Calculate Loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Getting gradients w.r.t. parameters\n",
    "        loss.backward()\n",
    "        \n",
    "        # Updating parameters\n",
    "        optimizer_lstm.step()\n",
    "        \n",
    "        iter += 1\n",
    "        \n",
    "        if iter % 500 == 0:\n",
    "            # Calculate Accuracy\n",
    "            correct = 0\n",
    "            total = 0       \n",
    "            \n",
    "            # Iterate through test dataset\n",
    "            for images, labels in test_loader:\n",
    "                images = Variable(images.view(-1, seq_dim, input_dim).to(device))\n",
    "            \n",
    "                # Forward pass only to get logits/output\n",
    "                outputs = model_lstm(images)\n",
    "                \n",
    "                # Get predictions from the maximum value\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                \n",
    "                # Total number of labels\n",
    "                total += labels.size(0)\n",
    "                \n",
    "                # Total correct predictions\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "            accuracy = 100 * correct / total\n",
    "                \n",
    "            \n",
    "            # Print Loss\n",
    "            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Classification\n",
    "We want to compare different model configurations with each other. \n",
    "\n",
    "For the RNN: \n",
    "* 1, 2, 3 or 4 hidden layers\n",
    "* tanh and ReLu activation function \n",
    "* Additional fully connected layer\n",
    "\n",
    "For the LSTM: \n",
    "* 1, 2 or 3 hidden layers\n",
    "* Additional fully connected layer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.1:\n",
    "Change the above implementation to allow for an efficient way to compare the final classification accuracies in one cell (i.e. define training methods and add model parameters). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN_Layers              1\n",
      "Activation           tanh\n",
      "Add_FC              False\n",
      "Epoch                   0\n",
      "Train Loss       1.453179\n",
      "Test Accuracy       66.44\n",
      "Name: 0, dtype: object\n",
      "RNN_Layers              1\n",
      "Activation           tanh\n",
      "Add_FC              False\n",
      "Epoch                   1\n",
      "Train Loss       0.728756\n",
      "Test Accuracy       73.15\n",
      "Name: 1, dtype: object\n",
      "RNN_Layers              1\n",
      "Activation           tanh\n",
      "Add_FC              False\n",
      "Epoch                   2\n",
      "Train Loss       0.392001\n",
      "Test Accuracy       83.09\n",
      "Name: 2, dtype: object\n",
      "RNN_Layers              1\n",
      "Activation           tanh\n",
      "Add_FC              False\n",
      "Epoch                   3\n",
      "Train Loss       0.393104\n",
      "Test Accuracy       91.93\n",
      "Name: 3, dtype: object\n",
      "RNN_Layers              1\n",
      "Activation           tanh\n",
      "Add_FC              False\n",
      "Epoch                   4\n",
      "Train Loss       0.294542\n",
      "Test Accuracy       93.09\n",
      "Name: 4, dtype: object\n",
      "RNN_Layers              1\n",
      "Activation           tanh\n",
      "Add_FC               True\n",
      "Epoch                   0\n",
      "Train Loss       1.476903\n",
      "Test Accuracy       75.61\n",
      "Name: 5, dtype: object\n",
      "RNN_Layers              1\n",
      "Activation           tanh\n",
      "Add_FC               True\n",
      "Epoch                   1\n",
      "Train Loss       0.637016\n",
      "Test Accuracy       88.34\n",
      "Name: 6, dtype: object\n",
      "RNN_Layers              1\n",
      "Activation           tanh\n",
      "Add_FC               True\n",
      "Epoch                   2\n",
      "Train Loss       0.392225\n",
      "Test Accuracy       90.11\n",
      "Name: 7, dtype: object\n",
      "RNN_Layers              1\n",
      "Activation           tanh\n",
      "Add_FC               True\n",
      "Epoch                   3\n",
      "Train Loss       0.294169\n",
      "Test Accuracy       93.11\n",
      "Name: 8, dtype: object\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 56\u001b[0m\n\u001b[0;32m     53\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[1;32m---> 56\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_rnn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer_rnn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     57\u001b[0m     test_acc \u001b[38;5;241m=\u001b[39m test_model(model_rnn, test_loader)\n\u001b[0;32m     58\u001b[0m     results \u001b[38;5;241m=\u001b[39m results\u001b[38;5;241m.\u001b[39m_append({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRNN_Layers\u001b[39m\u001b[38;5;124m'\u001b[39m: layer, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mActivation\u001b[39m\u001b[38;5;124m'\u001b[39m: act,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAdd_FC\u001b[39m\u001b[38;5;124m'\u001b[39m: fc,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch\u001b[39m\u001b[38;5;124m'\u001b[39m: epoch,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrain Loss\u001b[39m\u001b[38;5;124m'\u001b[39m: train_loss,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTest Accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m: test_acc}, ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[25], line 21\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, criterion, optimizer, train_loader)\u001b[0m\n\u001b[0;32m     18\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m     19\u001b[0m     running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m---> 21\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m running_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_loader)\n",
      "File \u001b[1;32mc:\\Users\\m-gre\\miniconda3\\envs\\AML_Tut\\lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\m-gre\\miniconda3\\envs\\AML_Tut\\lib\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# RNN Training\n",
    "# Number of steps to unroll\n",
    "seq_dim = 28\n",
    "input_dim = 28\n",
    "output_dim = 10\n",
    "learning_rate = 0.1\n",
    "\n",
    "def train_model(model, criterion, optimizer, train_loader):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.view(-1, seq_dim, input_dim).to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return running_loss / len(train_loader)\n",
    "\n",
    "def test_model(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images = images.view(-1, seq_dim, input_dim).to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    return 100 * correct / total\n",
    "\n",
    "layer_dim = [1, 2, 3, 4]\n",
    "activation = ['tanh', 'relu']\n",
    "add_fc = [False, True]\n",
    "\n",
    "results = pd.DataFrame(columns=['RNN_Layers', 'Activation', 'Add_FC','Epoch', 'Train Loss', 'Test Accuracy'])\n",
    "\n",
    "for layer in layer_dim:\n",
    "    for act in activation:\n",
    "        for fc in add_fc:\n",
    "            model_rnn = RNNModel(input_dim, 100, layer, fc, output_dim)\n",
    "            model_rnn.to(device)\n",
    "            optimizer_rnn = torch.optim.SGD(model_rnn.parameters(), lr=learning_rate)\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "            for epoch in range(num_epochs):\n",
    "                train_loss = train_model(model_rnn, criterion, optimizer_rnn, train_loader)\n",
    "                test_acc = test_model(model_rnn, test_loader)\n",
    "                results = results._append({'RNN_Layers': layer, 'Activation': act,'Add_FC': fc,'Epoch': epoch,'Train Loss': train_loss,'Test Accuracy': test_acc}, ignore_index=True)\n",
    "                print(results.iloc[-1])\n",
    "\n",
    "\n",
    "# Print the results\n",
    "for layer, act, fc, train_loss, test_acc in results:\n",
    "    print(f'Layer: {layer}, Activation: {act}, FC: {fc}, Train Loss: {train_loss}, Test Accuracy: {test_acc}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.2:\n",
    "Do your results differ from the results presented in the lecture? If so, why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your answer goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3:\n",
    "\n",
    "So far, we always trained for 3000 iterations with a batch size of 100 and a learning rate of 0.1. Our classification accuracies might be improved, if we change these values. Systematically change these values and find a better combination (if possible). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: \n",
    "1. Why might the LSTM result in better classification accuracies? What are the advantages and disadvantages of using an LSTM in this task, compared to an RNN?\n",
    "2. We addressed other variants of RNNs in the lecture. Which of them might be suitable for this classification task an why? (GRU, bidirectional RNN, Recursive Neural Network, Encoder-Decoder RNN)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
