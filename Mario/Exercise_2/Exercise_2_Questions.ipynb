{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Regularization\n",
    "\n",
    "1) Let's say, you're training a neural network to classify male and female faces. Your dataset is difficult: a human manages to achieve only a 15% accuracy on it. At different stages during the development of your model you achieve the following four train,test error tuples. \n",
    "\n",
    "Assign labels to each of the four tuples according to the bias/variance situation you're facing with that model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![biasVar_aufgabe.png](biasVar_aufgabe.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) What are your options to improve the performance of your model when your results are from the third column?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Explain the problem we're running into at test time when implementing Dropout purely as a randomized switch of unit activations (unit outputs) during training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) How can we solve the problem described in 3?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) What effect on neural network weights do Early Stopping, Dropout and L2 regularization have in common?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Optimization\n",
    "\n",
    "1) What is an optimization algorithm like Gradient Descent trying to do? Explain this by using the following terms: loss function, first-order partial derivitive, back-propagation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) What are three advantages of mini-batch gradient descent compared to stochastic GD and batch GD?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) What statistical principle do Momentum, RMSprop and Adam have in common? Explain this principle and its effect.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: End-to-End Systems\n",
    "\n",
    "1) Explain what an End-to-End Deep Learning System is by discussing an example in the context of Automatic Speech Recognition\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Consider the following task: The superhero Daredevil (who is blind) wants to be able to view CCTV camera footage (no audio) to help him solve crimes. He hires you and asks you to design a system that will be able to answer all his (spoken) questions about the contents of the footage.\n",
    "\n",
    "a) How would your system look like that is solving this task? Give a *rough* outline of both: an End-to-End system as well as a modular system (input/output and involved processes/modules (rough) for each system).\n",
    "\n",
    "b) What are the advantages/disadvantages of each of your systems you described in a)?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Convolutional Neural Networks\n",
    "\n",
    "1) Consider the first convolutional layer from the AlexNet architecture (lecture slide 46). Read of the following parameters from the figure: Kernel size and number of kernels. How many trainable weights does this layer consist of? Perform the calculation with respect to the values given in the figure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Consider the output volume of the last convolutional layer in the AlexNet network (lecture slide 46). Before further processing from the dense layers, the output volume gets processed by a max pooling layer. Based on the output volume shape (dimensions can be read off from the figure), what is the shape of the output volume after the max pool operation? The max pool layer uses a pool size of 3 x 3 and a stride of 2.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Compute the gradient for the kernel weights for a convolutional layer. The input from the forward pass is buffered and has the values [2, 3, 4, 5]. The kernel has a size of 3 and contains the following weights: [1, 2, 1]. The gradient dL/dR is given by [3, 4]. Compute the gradient dL/dK by carrying out the calculations for dL/dK = dL/dR * dR/dK. Check your results with the convolution approach for the backward pass.\n",
    "\n",
    "If we increase a component of K by 1, how does this change the output with respect to the gradient of dL/dK?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
