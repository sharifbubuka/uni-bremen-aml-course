{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AML Exercise Modern Transformer Architecture\n",
    "\n",
    "In the lecture it was mentioned that transformer architectures dominate in many domains. A recent architecture that uses transformers is ChatGPT. The system was roughly explained in the lecture. \n",
    "To further bridge the gap between the original transformer architecture shown in the lecture and modern architectures, we want you to look at another very new system: SAM (Segment Anything Model).\n",
    "\n",
    "Read the according paper, published in April 2023: \n",
    "[Segment Anything](https://arxiv.org/pdf/2304.02643.pdf). \n",
    "There is a [website](https://segment-anything.com/) available where you can try out the system (being run on a web server). Please do so.\n",
    "\n",
    "Use the paper to answer the following questions:\n",
    "\n",
    " \n",
    "1) What is the goal of the model and what makes it special?\n",
    "\n",
    "2) Explain how the architecture relates to a standard transformer architecture. Where can you identify components you know from the lecture and what is new?\n",
    "\n",
    "3) What is a foundation model? Name another foundation model that was covered in the lecture. \n",
    "\n",
    "4) Foundation models require huge amounts of labelled data. How do foundation models (like the one in your previous answer) typically solve this issue? \n",
    "\n",
    "5) Would a similar approach have been possible for SAM? Explain the strategy the authors of SAM used to overcome the data problem.\n",
    "\n",
    "6) Can SAM be used for Zero-Shot object classification? What if there are multiple objects in the image?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# AML Exercise Distance Metric Learning\n",
    "\n",
    "In this exercise sheet, we will review pair-based losses and distance-based losses.\n",
    "We will also look into a novel proxy-based loss for distance metric learning.\n",
    "Lastly, you will use prototypical networks to perform few-shot learning on the\n",
    "[Omniglot dataset](https://github.com/brendenlake/omniglot).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1 Pair-based vs. Proxy-based DML Methods\n",
    "\n",
    "In the lecture, you have encountered pair-based DML, e.g. the triplet loss, and proxy-based methods, e.g. the Proxy-NCA loss.\n",
    "Briefly describe the main difference the two DML methods and what the pros and cons are.\n",
    "Feel free to use section 2 of the paper of the next exercise: https://arxiv.org/pdf/2003.13911.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "`TODO Enter answers`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2 Proxy-Anchor-Loss\n",
    "The Proxy-NCA loss was one of the first proxy-based DML methods and was described in the lecture. In recent years, novel proxy-based losses have been introduced.\n",
    "One example of such a loss is the Proxy-Anchor loss, introduced by Kim et al.\n",
    "Read through section 3 of the paper [\n",
    "Kim et al. (2020): \"Proxy Anchor Loss for Deep Metric Learning\"](https://arxiv.org/pdf/2003.13911.pdf) and answer:\n",
    " - How does the Proxy-Anchor-loss work?\n",
    " - What are its benefits in comparison to Proxy NCA?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`TODO Enter answers`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Exercise 3 Few-Shot Learning using Prototypical Networks\n",
    "\n",
    "We will now look at Prototypical networks you have encountered in the lecture.\n",
    "In particular, we will try to replicate the experiments of section 3.1\n",
    "of the [paper which introduced prototypical networks](https://arxiv.org/pdf/1703.05175.pdf) .\n",
    "We will train on the\n",
    "[OmniGlot data set](https://github.com/brendenlake/omniglot), consisting of images of characters of 50 alphabets.\n",
    "30 alphabets belong to the \"background\" data set and should be used to train FSL models,\n",
    "while the remaining 20 alphabets are used for evaluation.\n",
    "The task of the Prototypical Network is to classify to which given character image samples query images belong to.\n",
    "Let's first load necessary imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import io\n",
    "import json\n",
    "\n",
    "import PIL\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torch.optim.adam import Adam\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision.transforms import ToTensor\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We will now load some data utilities we have set up. Make sure to load the corresponding files from StudIP.\n",
    "In the `prepare_data_set` method, the Omniglot data set is downloaded to a `data` directory next to this notebook.\n",
    "The images are also downscaled to 28x28 pixels and rotated four times (0, 90, 180, 270 degrees).\n",
    "The rotations build their own distinct classes.\n",
    "\n",
    "Note that this operation can take a while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '.')\n",
    "\n",
    "from prepare_data import OmniGlotDataSet, sample_from_torch_sample_dict\n",
    "DATA_DIR = Path(\".\") / \"data\"\n",
    "# Load example data\n",
    "omniglot_dataset = OmniGlotDataSet(DATA_DIR)\n",
    "omniglot_dataset.prepare_data_set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now setup our neural network encoder as defined in the paper. It is a Conv-Net using four blocks, each consisting of a 2D Conv, Relu, BatchNorm and MaxPool layer. It maps our 28 x 28 grayscale images to a 64-dimensional embedding vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from encoder import OmniGlotEncoder\n",
    "\n",
    "EMB_DIM = 64\n",
    "encoder = OmniGlotEncoder(EMB_DIM)\n",
    "print(encoder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.1 Implementing the ProtoNet Loss\n",
    "\n",
    "We will now get to the core of this exercise. You will need to implement the ProtoNet loss as described in the original paper.\n",
    "The `ProtoNet.calc_loss` methods has comments which will guide you through your implementation.\n",
    "We have setup a JSON file with pre-defined labels and their support / query sets, on which you can test your implementation (see the following cells).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProtoNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements a ProtoNet as introduced by Snell etl al. (2017): https://arxiv.org/pdf/1703.05175.pdf\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, encoder: nn.Module, emb_dim: int):\n",
    "        \"\"\"\n",
    "        Initializes the Protonet.\n",
    "        :encoder: A neural network which maps data samples to embedding vectors.\n",
    "        :emb_dim: The dimensionality of the embedding networks of the encoder.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.emb_dim = emb_dim\n",
    "\n",
    "    def calc_loss(self, support_set_batch: Tensor, query_set_batch: Tensor, device: torch.device) -> Tuple[Tensor, Tensor]:\n",
    "        \"\"\"\n",
    "        Calculates the loss for one training epoch of a prototypical network\n",
    "        :param support_set_batch: A tensor of shape: num_classes x num_support_samples_per_class  x embedding_dim\n",
    "        :param query_set_batch: A tensor of shape: num_classes x num_query_samples_per_class x embedding_dim\n",
    "        :param device: The device to which target values for the loss calculation should be put on.\n",
    "        :return A tuple of the loss and the accuracy for accurately predicting the true class of the query samples.\n",
    "        \"\"\"\n",
    "        # The dimensionality of the batches\n",
    "        num_classes, num_support = support_set_batch.shape[0], support_set_batch.shape[1]\n",
    "        num_query = query_set_batch.shape[1]\n",
    "        data_shape = query_set_batch.shape[2:]\n",
    "\n",
    "        # Step 1: Compute embeddings using the encoder\n",
    "        ## YOUR CODE START\n",
    "        all_data = None # Placeholder\n",
    "        ## S1.1. Merge support and query samples to one large batch, such that we can compute the embeddings faster using parallelization\n",
    "\n",
    "        ## S1.1.1 First \"unravel\" / reshape all support samples to one batch of shape: (num_classes * num_support, [data_shape])\n",
    "\n",
    "        ## S1.1.2 Also \"unravel\" the query batch to shape (num_classes * num_query, [data_shape])\n",
    "\n",
    "        ## S1.1.3 Concatenate the batches (support-set first) to the shape (num_classes * (num_support + num_query), data_shape)\n",
    "\n",
    "        ## YOUR CODE END\n",
    "        ## Step 1.2: Now compute the embeddings on the larger data batch\n",
    "        ## Step 1.2. Use the encoder to get the embeddings of all data samples\n",
    "        embeddings = self.encoder(all_data)\n",
    "        assert embeddings.shape == (num_classes * (num_support + num_query), self.emb_dim)\n",
    "\n",
    "        ## Step 1.3 Calculate the proto vectors (centroids)\n",
    "        ### Step 1.3.1 Get the support set  from the embeddings batch and reshape it to (num_classes, num_support, emb_dim)\n",
    "        ### YOUR CODE START\n",
    "        ### Step 1.3.2 Calculate the mean of the embeddings. This should result in a tensor of shape (num_classes, emb_dim)\n",
    "\n",
    "        ## Step 1.4 Calculate the distances between proto and query vectors\n",
    "        ### Step S.1.4.1 Get the query embeddings from the large data batch of step 5.3\n",
    "\n",
    "        ### Step S.1.4.2 Compute the pairwise euclidian distance between query embeddings and prototypical vectors\n",
    "        ### Tip: Use pytorch cdist\n",
    "\n",
    "        ## Step 1.5 Compute the loss of the protonet\n",
    "        ### Step 1.5.1 Apply the log_softmax on negative (!) distances;\n",
    "        ### Note: Be sure that the softmax is applied such that for each query embedding, there is a (log) prob distribution over the classes\n",
    "        ## Step 1.5.2: Multiply the log_probs with -1, as we aim to minimize the loss (but would maximize the log probs)\n",
    "        neg_log_probs = None # Placeholder\n",
    "\n",
    "        ### Step 1.5.2: Reshape the negative log probs to the shape (num_classes, num_query, num_classes)\n",
    "        ### I.e., we again have the same batch shape as the input (num_classes, num_query, ...rest),\n",
    "        ### but now, the rest is not the shape of the original data, but the log probs to each of the 12 classes\n",
    "\n",
    "        ### Step 1.5.3 We now setup target indicies representing the class labels\n",
    "        # We want to setup a (num_classes, num_query, 1) matrix which we then can use to get the negative log prob value\n",
    "        # of the actual target class of each sample\n",
    "        # For example, if we would have num_classes=5 and num_query=4, then the targets would be (as a 2d matrix)\n",
    "        # [[0 0 0 0]\n",
    "        #  [1 1 1 1]\n",
    "        #  [2 2 2 2]\n",
    "        #  [3 3 3 3]\n",
    "        #  [4 4 4 4]]\n",
    "        targets = None # Placeholder\n",
    "\n",
    "        ### Step 1.5.4 Use \"torch.gather\" along the dim=2 to get the index of the log probs of the target class of each sample\n",
    "\n",
    "        ### Step 1.5.5: Take the mean of the negative log probs of target classes. This is our loss\n",
    "        ### YOUR CODE END\n",
    "\n",
    "        # We also predict the classes of thq query samples using the smallest negated log likelihood\n",
    "        _, class_predictions = neg_log_probs.min(2)\n",
    "        accuracy = torch.eq(class_predictions, targets.squeeze()).float().mean()\n",
    "\n",
    "        return loss, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the ProtoNet Loss\n",
    "We will first test the untrained ProtoNet on the example data defined in the `example_data.json` uploaded on StudIP.\n",
    "Using an untrained ProtoNet, you should get an accuracy of approx 0.72 and a loss of approx 1.5.\n",
    "Note that due to implementation details, your loss might differ slightly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Example JSON\n",
    "example_json_file = Path(\".\") / \"example_data.json\"\n",
    "print_json_content = False  ## Change this if printed json contents are too large\n",
    "if example_json_file.exists():\n",
    "    print(f\"Loading example data json from {example_json_file.absolute()}\")\n",
    "    with open(example_json_file) as fp:\n",
    "        example_dict = json.load(fp)\n",
    "        if print_json_content:\n",
    "            print(\"Example JSON file content\")\n",
    "            print(json.dumps(example_dict, indent=4))\n",
    "\n",
    "else:\n",
    "    print(f\"Please download the 'example_data.json' file from StudIP.\")\n",
    "    sys.exit()\n",
    "example_support, example_query, example_classes = omniglot_dataset.load_example_label_to_img_dict(example_dict)\n",
    "\n",
    "\n",
    "# Init Protonet and test on example data\n",
    "DEVICE = \"cpu\"\n",
    "proto_net = ProtoNet(encoder, EMB_DIM)\n",
    "loss, acc = proto_net.calc_loss(example_support.to(DEVICE), example_query.to(DEVICE), DEVICE)\n",
    "print(f\"Example loss before loading: {loss}\")\n",
    "print(f\"Example accuracy before loading: {acc}\")\n",
    "\n",
    "assert loss > 1.0\n",
    "assert acc < 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing a Pre-trained ProtoNet\n",
    "\n",
    "We will now test the supplied `proto-net-release.pth`, which has been trained for 1000 iterations.\n",
    "You should see a significantly smaller loss (~0.016) and an accuracy of 100%. \n",
    "The exact loss might also differ because of implementation details. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = Path(\".\") / \"proto-net-release.pth\"\n",
    "state_dict = torch.load(MODEL_PATH)\n",
    "print(\"Loading protonet and optimizer state dicts\")\n",
    "proto_net.load_state_dict(state_dict[\"proto_net\"])\n",
    "\n",
    "loss, acc = proto_net.calc_loss(example_support.to(DEVICE), example_query.to(DEVICE), DEVICE)\n",
    "print(f\"Example loss after loading: {loss}\")\n",
    "print(f\"Example accuracy afer loading: {acc}\")\n",
    "\n",
    "assert np.isclose(acc, 1.0)\n",
    "assert loss < 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.3 Training the ProtoNet\n",
    "We will now train the ProtoNet for a small amount of epochs. You do not have to implement anything in this part. \n",
    "If your loss is correctly implemented, you should see a high accuracy (>95%) after training for one epoch of 2000 episodes.\n",
    "If you want to have the same settings as in the paper, you would need to set the number of epochs to 5 and train for 2000 episodes per epoch.\n",
    "Note, however, that one iteration is rather slow if you do not have a GPU.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing some training settings\n",
    "MAX_NUM_EPOCHS = 1  # Use 5 epochs for a setup like in the paper\n",
    "NUM_EPISODES_PER_EPOCH = 1000  # Use 2000 episodes for a setup like in the paper\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# Constants\n",
    "LEARNING_RATE = 10 ** -3\n",
    "NUM_CLASSES_PER_TRAIN_EPISODE = 60\n",
    "NUM_SUPPORT_SAMPLES = 5\n",
    "NUM_QUERY_SAMPLES = 5\n",
    "\n",
    "from random import seed\n",
    "# First we set some random seeds.\n",
    "seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "# Reset the network\n",
    "def weight_reset(m):\n",
    "    # https://discuss.pytorch.org/t/how-to-re-set-alll-parameters-in-a-network/20819/7\n",
    "    reset_parameters = getattr(m, \"reset_parameters\", None)\n",
    "    if callable(reset_parameters):\n",
    "        m.reset_parameters()\n",
    "\n",
    "print(\"Reseting weights of ProtoNet\")\n",
    "proto_net.apply(weight_reset)\n",
    "\n",
    "# # Setup a TB Writer\n",
    "LOG_DIR = Path(\".\")  / \"logs\"\n",
    "TB_DIR = LOG_DIR / \"tb\" / str(datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\"))\n",
    "TB_DIR.mkdir(exist_ok=True, parents=True)\n",
    "tb_writer = SummaryWriter(TB_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now load dictionaries which map the character classes to a list of tensors representing the pre-processed\n",
    "images.\n",
    "We load all images to memory such that training becomes faster. \n",
    "This operation also will take a while to finish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "USE_MULTI_PROC = False\n",
    "bg_label_to_img = omniglot_dataset.init_torch_sample_dict(part=\"background\", use_multi_proc=USE_MULTI_PROC)\n",
    "eval_label_to_img = omniglot_dataset.init_torch_sample_dict(part=\"evaluation\", use_multi_proc=USE_MULTI_PROC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Next, we setup the optimizer and a learning rate scheduler which halfs the learning rate every epoch.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 10 ** -3\n",
    "optim = Adam(params=proto_net.parameters(), lr=LEARNING_RATE)\n",
    "# A scheduler which halfs the learning rate\n",
    "scheduler = StepLR(optim, 1, gamma=0.5, last_epoch=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now start training, which will take a while. You can see the number of iterations per second in the progress bar. \n",
    "If your loss is correctly implemented, the loss should  decline, while the accuracy increases.\n",
    "Let the training script finish and post plots of  the training loss and accuracy (see tensorboard), as well as the eval results below.\n",
    "Note: When you test on the eval set, then the model sees samples of labels which were not used during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_epoch = 0\n",
    "global_step = 0\n",
    "\n",
    "bg_classes = list(sorted(bg_label_to_img.keys()))\n",
    "eval_classes = list(sorted(eval_label_to_img.keys()))\n",
    "\n",
    "print(\"Starting training...\")\n",
    "print(f\"See the progress in tensorboard. Run:\")\n",
    "print(f\"tensorboard --logdir {TB_DIR}\")\n",
    "\n",
    "while current_epoch < MAX_NUM_EPOCHS:\n",
    "    print(f\"Starting new train epoch: {current_epoch}/{MAX_NUM_EPOCHS}\")\n",
    "\n",
    "    print(f\"Starting new training loop of {NUM_EPISODES_PER_EPOCH} episodes\")\n",
    "    epoch_train_losses = []\n",
    "    epoch_train_accs = []\n",
    "    # Train Loop\n",
    "    proto_net.train()\n",
    "\n",
    "    print(\"Starting train loop\")\n",
    "    train_p_bar = tqdm(range(NUM_EPISODES_PER_EPOCH), total=NUM_EPISODES_PER_EPOCH)\n",
    "    for episode in train_p_bar:\n",
    "        support_set, query_set, sampled_classes = sample_from_torch_sample_dict(\n",
    "            bg_classes, bg_label_to_img, NUM_CLASSES_PER_TRAIN_EPISODE, NUM_SUPPORT_SAMPLES, NUM_QUERY_SAMPLES\n",
    "        )\n",
    "        optim.zero_grad()\n",
    "        loss, acc = proto_net.calc_loss(support_set.to(DEVICE), query_set.to(DEVICE), DEVICE)\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        # Log the loss and accuracy\n",
    "        loss_float = float(loss.item())\n",
    "        acc_float = float(acc.item())\n",
    "        tb_writer.add_scalar(\"train_step_loss\", loss_float, global_step)\n",
    "        tb_writer.add_scalar(\"train_step_acc\", acc_float, global_step)\n",
    "        epoch_train_losses.append(loss_float)\n",
    "        epoch_train_accs.append(acc_float)\n",
    "        global_step += 1\n",
    "\n",
    "        if episode % 10 == 0:\n",
    "            train_p_bar.set_postfix(accuracy=100. * np.mean(epoch_train_accs),\n",
    "                                    loss=np.mean(epoch_train_losses))\n",
    "\n",
    "    print(f\"Avg Train Loss: {np.mean(epoch_train_losses)}\")\n",
    "    print(f\"Train Acc. ({NUM_CLASSES_PER_TRAIN_EPISODE}-way {NUM_SUPPORT_SAMPLES}-Shot): : {np.mean(epoch_train_accs)}\")\n",
    "\n",
    "    tb_writer.add_scalar(\"epoch_avg_train_loss\", np.mean(epoch_train_losses))\n",
    "    tb_writer.add_scalar(\"epoch_avg_train_acc\", np.mean(epoch_train_accs))\n",
    "\n",
    "    # Half the learning rate\n",
    "    print(\"Halfing learning rate...\")\n",
    "    scheduler.step()\n",
    "    current_epoch += 1\n",
    "\n",
    "print(\"Finished training!\")\n",
    "# Test loop\n",
    "test_losses = []\n",
    "test_accs = []\n",
    "proto_net.eval()\n",
    "print(\"Starting to calculate eval results\")\n",
    "NUM_TESTING_EPISODES = 1000\n",
    "NUM_CLASSES_PER_TEST_EPISODE = 5\n",
    "\n",
    "test_p_bar = tqdm(range(NUM_TESTING_EPISODES), total=NUM_TESTING_EPISODES)\n",
    "for test_episode in test_p_bar:\n",
    "    with torch.no_grad():\n",
    "        support_set, query_set, sampled_classes = sample_from_torch_sample_dict(\n",
    "            eval_classes, eval_label_to_img, NUM_CLASSES_PER_TEST_EPISODE, NUM_SUPPORT_SAMPLES, NUM_QUERY_SAMPLES\n",
    "        )\n",
    "        loss, acc = proto_net.calc_loss(support_set.to(DEVICE), query_set.to(DEVICE), DEVICE)\n",
    "    # Log the loss and accuracy\n",
    "    test_losses.append(float(loss.item()))\n",
    "    test_accs.append(float(acc.item()))\n",
    "\n",
    "    if test_episode % 100 == 0:\n",
    "        test_p_bar.set_postfix(accuracy=100. * np.mean(test_accs), loss=np.mean(test_losses))\n",
    "\n",
    "\n",
    "print(f\"Test-Loss: {np.mean(test_losses)}\")\n",
    "print(f\"Test-Acc ({NUM_CLASSES_PER_TEST_EPISODE}-way {NUM_SUPPORT_SAMPLES}-Shot): {np.mean(test_accs)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`TODO Report test accuracy, tensorboard logs`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
