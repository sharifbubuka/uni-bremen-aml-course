{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditional Probabilities\n",
    "You know the University of Bremen has 18,631 students, of which 6,671 are in natural sciences and engineering (see https://www.uni-bremen.de/en/university/profile/facts-figures ). Three-quarters of your friends in the natural sciences like mate (a beverage) from your personal experience. \n",
    "You are curious if you can determine how likely someone studies in this field, given they like mate. Therefore, you conduct a quick experiment in the mensa and ask at random tables the field and how much they like mate. \n",
    "\n",
    "The following matrix describes your data. The first column describes if the person studies natural sciences (or not) and the second how much they like mate (scale from -2 to 2, higher=likes better, neutral is not allowed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  1],\n",
       "       [ 0, -1],\n",
       "       [ 0,  1],\n",
       "       [ 0, -1],\n",
       "       [ 1,  1],\n",
       "       [ 0,  1],\n",
       "       [ 0, -2],\n",
       "       [ 0, -1]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questionaire_mate = np.array([[True, 1], [False, -1], [False, 1], [False, -1], [True, 1], [False, 1], [False, -2], [False, -1]])\n",
    "questionaire_mate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a person likes mate, how likely are they to study in the natural sciences?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n"
     ]
    }
   ],
   "source": [
    "mate = questionaire_mate[:, 1]\n",
    "study_MINT = questionaire_mate[:, 0]\n",
    "    \n",
    "prob_mate = (mate > 0).sum() / len(questionaire_mate) \n",
    "prob_uni = ((study_MINT == True) & (mate > 0)).sum() / len(questionaire_mate)\n",
    "\n",
    "prob_MINT_given_mate = (prob_uni / prob_mate) \n",
    "print(prob_MINT_given_mate)\n",
    "\n",
    "## Thus, the friend sample seem to overrepesent the prop to study MINT given a person likes mate.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximum Likelihood Estimation\n",
    "A Gaussian normal distribution can be fitted by applying the Maximum Likelihood Estimation to determine the best parameters for explaining a given dataset. This is equivalent to calculating the mean (and variance) on the dataset directly; why?\n",
    "\n",
    "The Gaussian normal distribution is given as follows:\n",
    "\n",
    "$$\n",
    "N(\\mu,\\sigma) = \\frac{1}{\\sigma \\sqrt{2\\pi}} \n",
    "e^{-\\frac{1}{2}(\\frac{x-\\mu}{\\sigma})^2}\n",
    "$$\n",
    "\n",
    "Hint: The partial derivative is easier to compute when using a log likelihood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We assume a normal distrubution of likelihoods. Thus, when the data is normally distributed, the the distribution of likelihoods is the same than the data distribution.\n",
    "\n",
    "Goal of maximum likelihood is to find the optimal way to fit a distribution to the data. We want the location that maximizes the likelihood of observaing the weights we measured. Thus it is the maximum likelihood estimate for the mean.\n",
    "\n",
    "A normal distribution assumes the modus = mean = median. The variance in a normal distribution = 1\n",
    "\n",
    "We wanna compute the total prob. of oberserving all of the data, (joint prob. distribution of all observed data points). \n",
    "\n",
    "Assumption: Randomization -> the the total prob. of observing all of the data is the product of observing each data point individually (.i.e. the product of the marginal prob).)\n",
    "\n",
    "\n",
    "We need the product rule for differentiate of the maximum likelihood -> log likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Kullback-Leibler Divergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$D_{KL}(P|Q) = \\sum_x P(x)log(\\frac{P(x)}{Q(x)})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Calculate the KL divergence for two discrete distributions $P$ and $Q$ over events $A,B,C$. \n",
    "Calculate $D_{KL}(P|Q)$ and $D_{KL}(Q|P)$ and compare! \n",
    "\n",
    "| Distribution | A | B | C |\n",
    "| --- | --- | --- | --- |\n",
    "| P | 0.5 | 0.3 | 0.2 |\n",
    "| Q | 0.4 | 0.2 | 0.4 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0945818719775651\n",
      "0.10690843007666134\n"
     ]
    }
   ],
   "source": [
    "p = [0.5,0.3,0.2]\n",
    "q = [0.4,0.2,0.4]\n",
    "\n",
    "def kl_div(p,q):\n",
    "    prob = np.divide(p,q)\n",
    "    kl = np.sum(p*np.log(prob))\n",
    "    \n",
    "    return kl\n",
    "\n",
    "div_p = kl_div(p,q)\n",
    "div_q = kl_div(q,p)\n",
    "\n",
    "print(div_p)\n",
    "print(div_q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) For this task, assume for simplicity that $P$ and $Q$ are discrete distributions over two events $A,B$. \n",
    "\n",
    "i) For a given $P$, what $Q_{min}$ minimizes $D_{KL}(P|Q)$? Justify your answer!\n",
    "\n",
    "ii) For a given P, show that there is no upper bound for $D_{KL}(P|Q)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "div_p_p = kl_div(p,p)\n",
    "print(div_p_p)\n",
    "\n",
    "# the minimal value for q is p itself. There should be \"no differenec\" between the distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) What is the relationship between KL divergence and cross-entropy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Cross Entropy = Entropy + KL Divergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Feature Compression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume you want to perform classification with two classes, $A$ and $B$ in the feature space ${\\rm I\\!R}^{2n}$. We can assume that the two classes follow a normal distribution, with $\\mu_A = (\\mu_1, \\mu_2)$ and $\\mu_B = (\\mu_1, \\mu_3)$, with $\\mu_1, \\mu_2, \\mu_3 \\in {\\rm I\\!R}^n$. $\\Sigma$ is identical for both distributions, see below ($\\sigma \\in [0,1]$, $\\alpha \\approx 1$). You perform a Principal Component Analysis (PCA) for feature space transformation.\n",
    "\n",
    "$$\\sum =\n",
    "\\left(\n",
    "  \\begin{array}{ccc}\n",
    "  \\begin{array}{cc} \n",
    "\\sigma & \\alpha\\\\\n",
    "\\alpha & \\sigma\n",
    "\\end{array} & \\dots & 0  \\\\\n",
    "  \\vdots & \\ddots & \\vdots  \\\\\n",
    "  0 & \\dots & \\begin{array}{cc} \n",
    "\\sigma & \\alpha\\\\\n",
    "\\alpha & \\sigma\n",
    "\\end{array} \n",
    "\\end{array} \\right) \\in \\mathbb{R}^{2n \\times 2n} $$\n",
    "\n",
    "a) Without calculating the result, make a prediction about how the sorted sequence of Eigenvalues will look like! You do not need to give exact numbers, but sketch the graph of Eigenvalues by Eigenvector index. How many components do you anticipate to keep to retain most of the variance in the data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint: you may choose to implement and plot an example for this task.\n",
    "If so, np.random.multivariate_normal and sklearn.decomposition might come in handy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two classes are normally distributed. The first step in PCA is z-standardization. Leading to a standard normal distribution. Thus, both classes have the same mean and a std of 1. The two classes cannot be discriminated. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Is the number of features you answered for part a) representative of the minimum number of features required for discriminating the two classes? Justify your answer!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3: Logistic Regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression is a simple, but important classification technique (despite the name, it is not used for regression) for binary classification tasks. \n",
    "\n",
    "To classify a sample $x$, we:\n",
    "\n",
    "1. Calculate $z(x) = \\theta^Tx$ (to include a bias term, add a constant feature 1 to $x$).\n",
    "2. Apply $h(x)=\\sigma(z(x))$ with $\\sigma(s)=\\frac{1}{1+e^{-s}}$  \n",
    "3. Apply a threshold $t$ to $h(x)$ to discriminate between the two classes (i.e., assign class 0 to $x \\iff h(x) < t$)\n",
    "\n",
    "For training, we initialize $\\theta$ randomly and perform gradient descent, i.e., loop over the following steps:\n",
    "\n",
    "1. Calculate the loss $J(\\theta)$ on the training data with $J(\\theta) = -y_1 \\cdot log(p_1) - (1-y_1) \\cdot log(1-p_1)$\n",
    "2. Adjust the weights $\\theta$ in the direction of $\\frac{\\delta J}{\\delta \\theta}$ with a learning rate of $l$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Argue why logistic regression can be considered a special case of a neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In linear regression the sigmoid function is used as a link function. In simple neural network, similar to logist regrssion the weight are summed up and then pluggend into the sigmoid function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Assume the logistic regression to detect a target class among non-targets. Describe how you can adjust the algorithm depending on whether a high recall or a high precision are more important in your application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Its matter of how high we set the probability threshold, whether we classify an instance to our target class or not. \n",
    "\n",
    "For high precision: we set the threshold higher. We want as little false positives. Thus, setting the porb. threshold high we dont run the risk of errornously classifying instances that do not belong to the target class. The denumerater should be as small as possible in the recall formular = TP / TP + FP. \n",
    "\n",
    "For high recall: we set the prob. threshold lower. We want as little false negatives as possible. Thus, setting the prob threshold lower decreases the risk of false negatives. Recall = TP / TP + FN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Program a classifier object LogisticRegression with methods fit(X,y) and predict(X) that implements training and classification as described above. While you should use PyTorch in all following programming tasks, use only elementary Python and numpy methods. For this purpose, you will need to determine the partial derivative of $J(\\theta)$. Fill out the following skeleton class for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPS = 1e-12\n",
    "\n",
    "class MyLogisticRegression:\n",
    "    def __init__(self, lr=0.01, num_iter=100000, verbose=False):\n",
    "        self.lr = lr\n",
    "        self.num_iter = num_iter\n",
    "        self.verbose = verbose\n",
    "        self.weights = None\n",
    "        self.loss_history = []\n",
    "        self.threshold = 0.5\n",
    "    \n",
    "    def __add_intercept(self, X):\n",
    "        intercept = np.ones((X.shape[0], 1))\n",
    "        return np.concatenate((intercept, X), axis=1)\n",
    "    \n",
    "    def __sigmoid(self, z):\n",
    "        z = h = np.clip(z, EPS, 1-EPS)\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    def __loss(self, h, y):\n",
    "        h = np.clip(h, EPS, 1-EPS)\n",
    "        return (-y * np.log(h) - (1 - y) * np.log(1 - h)).mean()\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        X = self.__add_intercept(X)\n",
    "        self.theta = np.zeros(X.shape[1])\n",
    "        \n",
    "        for i in range(self.num_iter):\n",
    "            z = np.dot(X, self.theta)\n",
    "            h = self.__sigmoid(z)\n",
    "            gradient = np.dot(X.T, (h - y)) / y.size\n",
    "            self.theta -= self.lr * gradient \n",
    "            \n",
    "            # Calculate and store the loss\n",
    "            loss = self.__loss(h, y)\n",
    "            self.loss_history.append(loss)\n",
    "            \n",
    "            if self.verbose and i % 10000 == 0:\n",
    "                # z = np.dot(X, self.weights)\n",
    "                # h = self.__sigmoid(z)\n",
    "                # loss = self.__loss(h, y)\n",
    "                print(f'Iteration {i}, Loss: {loss}')\n",
    "     \n",
    "    def predict_prob(self, X):\n",
    "        X = self.__add_intercept(X)\n",
    "        z = np.dot(X, self.theta)\n",
    "        return self.__sigmoid(z)\n",
    "    \n",
    "    def predict(self, X, threshold):\n",
    "        \n",
    "        return self.predict_prob(X) >= threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "(X,y) = load_breast_cancer(return_X_y=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=16)\n",
    "\n",
    "clf = MyLogisticRegression()\n",
    "\n",
    "clf.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0 52]\n",
      " [ 0 91]]\n",
      "0.6363636363636364\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "y_pred = clf.predict(X_test, threshold = 0.5)\n",
    "                      \n",
    "cnf_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
    "print(cnf_matrix)\n",
    "\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) Evaluate your logistic regression classifier with the BreastCancer data set (available in scikit-learn). The optimization problem during training of logistic regression is convex, i.e., it will always converge towards a global minimum. How can you verify this empirically?\n",
    "\n",
    "Hint: if you had trouble implementing the logistic regression earlier, you may use the sklearn version here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(clf.loss_history)\n",
    "plt.title('Training Loss History')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/maraweber/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "(X,y) = load_breast_cancer(return_X_y=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=16)\n",
    "\n",
    "model = LogisticRegression()\n",
    "\n",
    "model.fit(X,y)\n",
    "# clf.predict_prob(X)\n",
    "pred = model.predict(X)\n",
    "\n",
    "# clf = LogisticRegression().fit(X,y)\n",
    "# clf.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
